{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b75185d",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34460bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black # black formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d790e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed2ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import random\n",
    "import os\n",
    "from functools import partial\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from boto3.session import Session\n",
    "from botocore.exceptions import ClientError\n",
    "from anthropic_bedrock import AnthropicBedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AnthropicBedrock() # AnthropicBedrock client is used for token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac39e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI key is used for AlpacaEval 2.0 evaluation\n",
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "   <OPEN-AI_API-KEY>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad82368",
   "metadata": {},
   "source": [
    "## Mixture of Agents Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055f5c3",
   "metadata": {},
   "source": [
    "In this scenario, we will be employing a two-layered Mixture of Experts (MoE) architecture. The initial two layers comprise proposers, each consisting of three Large Language Models (LLMs). These LLMs have the capability to operate with varying inference parameters, enabling them to propose a diverse range of responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519fa612",
   "metadata": {},
   "source": [
    "![architecture](architecture-simple.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc5ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three Amazon Bedrock FMs as proposers\n",
    "reference_models = [\n",
    "    {\n",
    "        \"modelId\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        \"inference_params\": {\"temperature\": 0.5, \"topP\": 1.0, \"top_k\": 250},\n",
    "        \"maxTokens\": 512,\n",
    "    },\n",
    "    {\n",
    "        \"modelId\": \"mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "        \"inference_params\": {\"temperature\": 0.7, \"topP\": 1.0},\n",
    "        \"maxTokens\": 512,\n",
    "    },\n",
    "    {\n",
    "        \"modelId\": \"us.meta.llama3-2-3b-instruct-v1:0\",\n",
    "        \"inference_params\": {\"temperature\": 0.5, \"topP\": 0.9},\n",
    "        \"maxTokens\": 512,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Single Amazon Bedrock FMs as aggregator\n",
    "\n",
    "aggregator_model = {\n",
    "    \"modelId\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"inference_params\": {\"temperature\": 0.0, \"topP\": 1.0, \"top_k\": 250},\n",
    "    \"maxTokens\": 512,\n",
    "}\n",
    "\n",
    "# Aggregator prompt\n",
    "aggreagator_system_prompt = \"\"\"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability. Do not write in response that this was synthesised from previous responses\n",
    "\n",
    "Responses from models:\"\"\"\n",
    "\n",
    "# 2 Layer MoA Architecture\n",
    "layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d788a",
   "metadata": {},
   "source": [
    "The objective here is to leverage less computationally intensive and more cost-effective models for the Mode of Action (MoA) task. We will utilize <b>Anthropic Claude 3 Haiku</b> as proposer and aggregator, <b>Llama 3.2 3B Instruct model</b> as proposer, and <b>Mistral 8x7B Instruct model</b> as proposer. Finally will evaluate the performance of such a MoA architecture against Anthropic's Claude 3.5 model in terms of response accuracy (as measured by AlpacaEval 2.0), computational cost, and latency. You can view the AlpacaEval 2.0 leaderboard [here](https://tatsu-lab.github.io/alpaca_eval/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a718c",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534fa880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFinalSystemPrompt(system_prompt, results):\n",
    "    \"\"\"\n",
    "    Constructs a system prompt for layers 2+ that includes the previous responses to synthesize.\n",
    "\n",
    "    Args:\n",
    "        system_prompt (str): The initial system prompt.\n",
    "        results (list): A list of tuples, where each tuple contains a response and its associated metadata.\n",
    "\n",
    "    Returns:\n",
    "        str: The final system prompt with the previous responses appended, or None if any of the responses are empty.\n",
    "    \"\"\"\n",
    "    for i, element in enumerate(results):\n",
    "        if len(element[0]) == 0:\n",
    "            return None\n",
    "    return (\n",
    "        system_prompt\n",
    "        + \"\\n\"\n",
    "        + \"\\n\".join(\n",
    "            [\n",
    "                f\"<Response_{i+1}> {str(element[0])} </Response_{i+1}> \\n\\n\"\n",
    "                for i, element in enumerate(results)\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5298e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability. Do not write in response that this was synthesised from previous responses\n",
      "\n",
      "Responses from models:\n",
      "<Response_1> Here are 3 fun things to do in San Francisco:\n",
      "\n",
      "1. Visit the Golden Gate Bridge - This iconic landmark offers stunning views of the city and the bay. You can walk or bike across the bridge for an up-close experience.\n",
      "\n",
      "2. Explore Fisherman's Wharf - This lively waterfront area is home to seafood restaurants, street performers, the Musée Mécanique arcade, and Pier 39 with its sea lion colony.\n",
      "\n",
      "3. Ride the Cable Cars - San Francisco's historic cable cars are a fun and unique way to get around the hilly city. You can ride the cable cars up and down the steep streets.\n",
      "\n",
      "Some other popular activities in San Francisco include visiting the colorful murals of the Mission District, touring Alcatraz Island, checking out the views from Twin Peaks, and exploring the shops and restaurants in neighborhoods like North Beach and the Castro. There are lots of fun and unique experiences to be had in this vibrant city. </Response_1> \n",
      "\n",
      "\n",
      "<Response_2>  Sure, I'd be happy to help! San Francisco is a city with no shortage of fun activities. Here are three fun things you might consider doing:\n",
      "\n",
      "1. Visit the Exploratorium: The Exploratorium is a hands-on science museum located on the Embarcadero. It features over 650 interactive exhibits that cover a wide range of topics, from physics and biology to art and psychology. You can play with optical illusions, create your own mini-tornado, or explore the world of microbes. It's a great place to learn and have fun at the same time.\n",
      "2. Take a Ferry to Sausalito: Sausalito is a charming town located across the bay from San Francisco. You can take a ferry from the Ferry Building or Pier 39 and enjoy the scenic views of the Bay and the Golden Gate Bridge. Once you arrive in Sausalito, you can explore the town's many art galleries, boutiques, and restaurants. You can also rent a bike and ride along the waterfront.\n",
      "3. Visit the California Academy of Sciences: The California Academy of Sciences is a natural history museum located in Golden Gate Park. It features an aquarium, a planetarium, a rainforest dome, and a natural history museum. You can learn about the diverse ecosystems of California, see penguins and other marine life, and take a journey through the stars in the planetarium. It's a great place to spend a day exploring and learning about the natural world.\n",
      "\n",
      "I hope these suggestions are helpful and that you have a great time in San Francisco! </Response_2> \n",
      "\n",
      "\n",
      "<Response_3> San Francisco is a vibrant city with endless options for entertainment and exploration. Here are 3 fun things to do in SF:\n",
      "\n",
      "1. **Explore Fisherman's Wharf and Pier 39**: This iconic waterfront district is known for its stunning views of the Golden Gate Bridge, sea lions at Pier 39, and a variety of seafood restaurants. You can also take a stroll along the pier, visit the Aquarium of the Bay, or ride the historic cable cars.\n",
      "\n",
      "2. **Take a stroll across the Golden Gate Bridge**: This iconic suspension bridge is a must-visit attraction in SF. You can walk, bike, or drive across the bridge for breathtaking views of the San Francisco Bay, Alcatraz Island, and the city skyline. If you're feeling adventurous, you can even take a guided tour or bike ride across the bridge.\n",
      "\n",
      "3. **Visit Alcatraz Island**: Once a notorious maximum-security prison, Alcatraz Island is now a popular tourist attraction. Take a ferry to the island and explore the former prison cells, listen to a guided audio tour, or take a self-guided tour of the island. You can also see the famous \"Cell Block C\" and learn about the island's infamous history.\n",
      "\n",
      "These are just a few of the many fun things to do in SF. Whether you're interested in history, nature, or entertainment, there's something for everyone in this vibrant city! </Response_3> \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        setTimeout(function() {\n",
       "            var nbb_cell_id = 8;\n",
       "            var nbb_formatted_code = \"print(\\n    getFinalSystemPrompt(\\n        system_prompt=aggreagator_system_prompt,\\n        results=[\\n            (\\n                \\\"Here are 3 fun things to do in San Francisco:\\\\n\\\\n1. Visit the Golden Gate Bridge - This iconic landmark offers stunning views of the city and the bay. You can walk or bike across the bridge for an up-close experience.\\\\n\\\\n2. Explore Fisherman's Wharf - This lively waterfront area is home to seafood restaurants, street performers, the Mus\\u00e9e M\\u00e9canique arcade, and Pier 39 with its sea lion colony.\\\\n\\\\n3. Ride the Cable Cars - San Francisco's historic cable cars are a fun and unique way to get around the hilly city. You can ride the cable cars up and down the steep streets.\\\\n\\\\nSome other popular activities in San Francisco include visiting the colorful murals of the Mission District, touring Alcatraz Island, checking out the views from Twin Peaks, and exploring the shops and restaurants in neighborhoods like North Beach and the Castro. There are lots of fun and unique experiences to be had in this vibrant city.\\\",\\n                22,\\n                214,\\n            ),\\n            (\\n                \\\" Sure, I'd be happy to help! San Francisco is a city with no shortage of fun activities. Here are three fun things you might consider doing:\\\\n\\\\n1. Visit the Exploratorium: The Exploratorium is a hands-on science museum located on the Embarcadero. It features over 650 interactive exhibits that cover a wide range of topics, from physics and biology to art and psychology. You can play with optical illusions, create your own mini-tornado, or explore the world of microbes. It's a great place to learn and have fun at the same time.\\\\n2. Take a Ferry to Sausalito: Sausalito is a charming town located across the bay from San Francisco. You can take a ferry from the Ferry Building or Pier 39 and enjoy the scenic views of the Bay and the Golden Gate Bridge. Once you arrive in Sausalito, you can explore the town's many art galleries, boutiques, and restaurants. You can also rent a bike and ride along the waterfront.\\\\n3. Visit the California Academy of Sciences: The California Academy of Sciences is a natural history museum located in Golden Gate Park. It features an aquarium, a planetarium, a rainforest dome, and a natural history museum. You can learn about the diverse ecosystems of California, see penguins and other marine life, and take a journey through the stars in the planetarium. It's a great place to spend a day exploring and learning about the natural world.\\\\n\\\\nI hope these suggestions are helpful and that you have a great time in San Francisco!\\\",\\n                30,\\n                353,\\n            ),\\n            (\\n                \\\"San Francisco is a vibrant city with endless options for entertainment and exploration. Here are 3 fun things to do in SF:\\\\n\\\\n1. **Explore Fisherman's Wharf and Pier 39**: This iconic waterfront district is known for its stunning views of the Golden Gate Bridge, sea lions at Pier 39, and a variety of seafood restaurants. You can also take a stroll along the pier, visit the Aquarium of the Bay, or ride the historic cable cars.\\\\n\\\\n2. **Take a stroll across the Golden Gate Bridge**: This iconic suspension bridge is a must-visit attraction in SF. You can walk, bike, or drive across the bridge for breathtaking views of the San Francisco Bay, Alcatraz Island, and the city skyline. If you're feeling adventurous, you can even take a guided tour or bike ride across the bridge.\\\\n\\\\n3. **Visit Alcatraz Island**: Once a notorious maximum-security prison, Alcatraz Island is now a popular tourist attraction. Take a ferry to the island and explore the former prison cells, listen to a guided audio tour, or take a self-guided tour of the island. You can also see the famous \\\\\\\"Cell Block C\\\\\\\" and learn about the island's infamous history.\\\\n\\\\nThese are just a few of the many fun things to do in SF. Whether you're interested in history, nature, or entertainment, there's something for everyone in this vibrant city!\\\",\\n                49,\\n                284,\\n            ),\\n        ],\\n    )\\n)\";\n",
       "            var nbb_cells = Jupyter.notebook.get_cells();\n",
       "            for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                    nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                    break;\n",
       "                }\n",
       "            }\n",
       "        }, 500);\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\n",
    "    getFinalSystemPrompt(\n",
    "        system_prompt=aggreagator_system_prompt,\n",
    "        results=[\n",
    "            (\n",
    "                \"Here are 3 fun things to do in San Francisco:\\n\\n1. Visit the Golden Gate Bridge - This iconic landmark offers stunning views of the city and the bay. You can walk or bike across the bridge for an up-close experience.\\n\\n2. Explore Fisherman's Wharf - This lively waterfront area is home to seafood restaurants, street performers, the Musée Mécanique arcade, and Pier 39 with its sea lion colony.\\n\\n3. Ride the Cable Cars - San Francisco's historic cable cars are a fun and unique way to get around the hilly city. You can ride the cable cars up and down the steep streets.\\n\\nSome other popular activities in San Francisco include visiting the colorful murals of the Mission District, touring Alcatraz Island, checking out the views from Twin Peaks, and exploring the shops and restaurants in neighborhoods like North Beach and the Castro. There are lots of fun and unique experiences to be had in this vibrant city.\",\n",
    "                22,\n",
    "                214,\n",
    "            ),\n",
    "            (\n",
    "                \" Sure, I'd be happy to help! San Francisco is a city with no shortage of fun activities. Here are three fun things you might consider doing:\\n\\n1. Visit the Exploratorium: The Exploratorium is a hands-on science museum located on the Embarcadero. It features over 650 interactive exhibits that cover a wide range of topics, from physics and biology to art and psychology. You can play with optical illusions, create your own mini-tornado, or explore the world of microbes. It's a great place to learn and have fun at the same time.\\n2. Take a Ferry to Sausalito: Sausalito is a charming town located across the bay from San Francisco. You can take a ferry from the Ferry Building or Pier 39 and enjoy the scenic views of the Bay and the Golden Gate Bridge. Once you arrive in Sausalito, you can explore the town's many art galleries, boutiques, and restaurants. You can also rent a bike and ride along the waterfront.\\n3. Visit the California Academy of Sciences: The California Academy of Sciences is a natural history museum located in Golden Gate Park. It features an aquarium, a planetarium, a rainforest dome, and a natural history museum. You can learn about the diverse ecosystems of California, see penguins and other marine life, and take a journey through the stars in the planetarium. It's a great place to spend a day exploring and learning about the natural world.\\n\\nI hope these suggestions are helpful and that you have a great time in San Francisco!\",\n",
    "                30,\n",
    "                353,\n",
    "            ),\n",
    "            (\n",
    "                \"San Francisco is a vibrant city with endless options for entertainment and exploration. Here are 3 fun things to do in SF:\\n\\n1. **Explore Fisherman's Wharf and Pier 39**: This iconic waterfront district is known for its stunning views of the Golden Gate Bridge, sea lions at Pier 39, and a variety of seafood restaurants. You can also take a stroll along the pier, visit the Aquarium of the Bay, or ride the historic cable cars.\\n\\n2. **Take a stroll across the Golden Gate Bridge**: This iconic suspension bridge is a must-visit attraction in SF. You can walk, bike, or drive across the bridge for breathtaking views of the San Francisco Bay, Alcatraz Island, and the city skyline. If you're feeling adventurous, you can even take a guided tour or bike ride across the bridge.\\n\\n3. **Visit Alcatraz Island**: Once a notorious maximum-security prison, Alcatraz Island is now a popular tourist attraction. Take a ferry to the island and explore the former prison cells, listen to a guided audio tour, or take a self-guided tour of the island. You can also see the famous \\\"Cell Block C\\\" and learn about the island's infamous history.\\n\\nThese are just a few of the many fun things to do in SF. Whether you're interested in history, nature, or entertainment, there's something for everyone in this vibrant city!\",\n",
    "                49,\n",
    "                284,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791537f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Backoff mechanism\n",
    "def backoff_mechanism(\n",
    "    func, modelId, inference_params, maxTokens, messages, prev_response, debug=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements an exponential backoff mechanism for retrying a function call.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The function to be called with retries. This will be invoke model function.\n",
    "        modelId (str): The ID of the model to use.\n",
    "        inference_params (dict): Parameters for the inference request.\n",
    "        maxTokens (int): The maximum number of tokens to generate.\n",
    "        messages (list): A list of messages to be used as context.\n",
    "        prev_response (str): The previous response from the model.\n",
    "        debug (bool, optional): Whether to print debug messages. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the response from the function, the number of tokens generated, and the cost of the request.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error other than a ThrottlingException occurs after the maximum number of retries.\n",
    "    \"\"\"\n",
    "    MAX_RETRIES = 5  # Maximum number of retries\n",
    "    INITIAL_DELAY = 1  # Initial delay in seconds\n",
    "    MAX_DELAY = 60  # Maximum delay in second\n",
    "\n",
    "    delay = INITIAL_DELAY\n",
    "    retries = 0\n",
    "\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            return func(\n",
    "                modelId=modelId,\n",
    "                inference_params=inference_params,\n",
    "                maxTokens=maxTokens,\n",
    "                messages=messages,\n",
    "                prev_response=prev_response,\n",
    "                debug=debug,\n",
    "            )\n",
    "        except ClientError as exception_obj:\n",
    "            if exception_obj.response[\"Error\"][\"Code\"] == \"ThrottlingException\":\n",
    "                print(f\"Retry {retries + 1}/{MAX_RETRIES}\")\n",
    "                time.sleep(delay + random.uniform(0, 1))  # Add a random jitter\n",
    "                delay = min(delay * 2, MAX_DELAY)\n",
    "                retries += 1\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        print(\"Max retries reached!\")\n",
    "        return \"\", 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(\n",
    "    modelId, inference_params, maxTokens, messages, prev_response, debug=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Invoke a model for inference.\n",
    "\n",
    "    Args:\n",
    "        modelId (str): The ID of the model to invoke.\n",
    "        inference_params (dict): A dictionary containing inference parameters such as temperature and top-p.\n",
    "        maxTokens (int): The maximum number of tokens to generate.\n",
    "        messages (list): A list of messages to pass to the model.\n",
    "        prev_response (str): The previous response from the model, if any.\n",
    "        debug (bool, optional): Whether to print debug information. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the model's response (str), the number of input tokens (int), and the number of output tokens (int).\n",
    "    \"\"\"\n",
    "    bedrock = Session().client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Invoking {modelId}\")\n",
    "\n",
    "    system_prompt = (\n",
    "        getFinalSystemPrompt(aggreagator_system_prompt, prev_response)\n",
    "        if prev_response\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if system_prompt is None:\n",
    "        return \"\", 0, 0\n",
    "\n",
    "    respone = None\n",
    "    if modelId in (\n",
    "        \"mistral.mistral-7b-instruct-v0:2\",\n",
    "        \"mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "    ):\n",
    "\n",
    "        if prev_response:\n",
    "            messages[0][\"content\"][0][\n",
    "                \"text\"\n",
    "            ] = f\"<s>[INST] {system_prompt} \\n\\n {messages[0]['content'][0]['text']} [/INST]\"\n",
    "        else:\n",
    "            messages[0][\"content\"][0][\n",
    "                \"text\"\n",
    "            ] = f\"<s>[INST] {messages[0]['content'][0]['text']} [/INST]\"\n",
    "\n",
    "        response = bedrock.converse(\n",
    "            modelId=modelId,\n",
    "            messages=messages,\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": maxTokens,\n",
    "                \"temperature\": inference_params[\"temperature\"],\n",
    "                \"topP\": inference_params[\"topP\"],\n",
    "            },\n",
    "            additionalModelRequestFields={\n",
    "                f\"{key}\": inference_params[key]\n",
    "                for key in inference_params.keys()\n",
    "                if key not in (\"temperature\", \"topP\")\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        response = bedrock.converse(\n",
    "            modelId=modelId,\n",
    "            messages=messages,\n",
    "            system=([{\"text\": system_prompt}] if prev_response else []),\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": maxTokens,\n",
    "                \"temperature\": inference_params[\"temperature\"],\n",
    "                \"topP\": inference_params[\"topP\"],\n",
    "            },\n",
    "            additionalModelRequestFields={\n",
    "                f\"{key}\": inference_params[key]\n",
    "                for key in inference_params.keys()\n",
    "                if key not in (\"temperature\", \"topP\")\n",
    "            },\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        response[\"output\"][\"message\"][\"content\"][0][\"text\"],\n",
    "        response[\"usage\"][\"inputTokens\"],\n",
    "        response[\"usage\"][\"outputTokens\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ae1ead9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Hello! I\\'m Claude, an AI assistant created by Anthropic. I don\\'t actually have a name like \"Sonnet.\" How can I help you today?',\n",
       " 15,\n",
       " 38)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        setTimeout(function() {\n",
       "            var nbb_cell_id = 11;\n",
       "            var nbb_formatted_code = \"prompt = \\\"Hi, Sonnet\\\"\\nmessages = [{\\\"role\\\": \\\"user\\\", \\\"content\\\": [{\\\"text\\\": f\\\"User Query: {prompt}\\\"}]}]\\nbackoff_mechanism(\\n    func=invoke_model,\\n    modelId=\\\"anthropic.claude-3-sonnet-20240229-v1:0\\\",\\n    inference_params={\\\"temperature\\\": 0.5, \\\"topP\\\": 1.0, \\\"top_k\\\": 250},\\n    maxTokens=512,\\n    prev_response=\\\"\\\",\\n    messages=messages,\\n)\";\n",
       "            var nbb_cells = Jupyter.notebook.get_cells();\n",
       "            for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                    nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                    break;\n",
       "                }\n",
       "            }\n",
       "        }, 500);\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Hi, Sonnet\"\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": f\"User Query: {prompt}\"}]}]\n",
    "backoff_mechanism(\n",
    "    func=invoke_model,\n",
    "    modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    inference_params={\"temperature\": 0.5, \"topP\": 1.0, \"top_k\": 250},\n",
    "    maxTokens=512,\n",
    "    prev_response=\"\",\n",
    "    messages=messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b99b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_llms(\n",
    "    modelId, inference_params, maxTokens, messages, prev_response=None, debug=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a parrallel LLM call to multiple models while accounting for previous responses + rate limits.\n",
    "    \n",
    "    Args:\n",
    "        modelId (str): The ID of the LLM model to use.\n",
    "        inference_params (dict): Parameters for the LLM inference.\n",
    "        maxTokens (int): The maximum number of tokens to generate.\n",
    "        messages (list): A list of messages to use as input for the LLM.\n",
    "        prev_response (str, optional): The previous response from the LLM. Defaults to None.\n",
    "        debug (bool, optional): Whether to enable debug mode. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the LLM.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    return await asyncio.to_thread(\n",
    "        backoff_mechanism,\n",
    "        invoke_model,\n",
    "        modelId,\n",
    "        inference_params,\n",
    "        maxTokens,\n",
    "        messages,\n",
    "        prev_response,\n",
    "        debug,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca93896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Hello! I\\'m Claude, an AI assistant created by Anthropic. I don\\'t have a specific name like \"Sonnet\", but I\\'m happy to chat with you. How can I help you today?',\n",
       " 15,\n",
       " 47)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        setTimeout(function() {\n",
       "            var nbb_cell_id = 13;\n",
       "            var nbb_formatted_code = \"await run_llms(\\n    modelId=\\\"anthropic.claude-3-sonnet-20240229-v1:0\\\",\\n    inference_params={\\\"temperature\\\": 0.5, \\\"topP\\\": 1.0, \\\"top_k\\\": 250},\\n    maxTokens=512,\\n    prev_response=\\\"\\\",\\n    messages=messages,\\n)\";\n",
       "            var nbb_cells = Jupyter.notebook.get_cells();\n",
       "            for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                    nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                    break;\n",
       "                }\n",
       "            }\n",
       "        }, 500);\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await run_llms(\n",
    "    modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    inference_params={\"temperature\": 0.5, \"topP\": 1.0, \"top_k\": 250},\n",
    "    maxTokens=512,\n",
    "    prev_response=\"\",\n",
    "    messages=messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff419c",
   "metadata": {},
   "source": [
    "## Mixture of Agents - Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "819e8a24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking anthropic.claude-3-haiku-20240307-v1:0\n",
      "Invoking mistral.mixtral-8x7b-instruct-v0:1\n",
      "Invoking us.meta.llama3-2-3b-instruct-v1:0\n",
      "Invoking us.meta.llama3-2-3b-instruct-v1:0\n",
      "Invoking mistral.mixtral-8x7b-instruct-v0:1\n",
      "Invoking anthropic.claude-3-haiku-20240307-v1:0\n",
      "Invoking anthropic.claude-3-haiku-20240307-v1:0\n",
      "Final Response:\n",
      "\n",
      "After carefully reviewing the responses provided by the various open-source models, I have synthesized the information into a comprehensive and accurate response to your query about fun things to do in San Francisco.\n",
      "\n",
      "1. Visit the iconic Golden Gate Bridge: This renowned landmark is a must-see attraction in San Francisco. You can walk, bike, or drive across the bridge to take in the stunning views of the bay and the city skyline. The bridge offers a classic San Francisco experience and provides excellent photo opportunities.\n",
      "\n",
      "2. Explore Fisherman's Wharf and Pier 39: This lively waterfront district is filled with seafood restaurants, street performers, and popular attractions like the Musée Mécanique arcade and the Aquarium of the Bay. You can also see the famous sea lions that reside at Pier 39, making it a fun and engaging destination for visitors.\n",
      "\n",
      "3. Ride the historic cable cars: San Francisco's cable car system is a unique and iconic mode of transportation that offers a delightful way to experience the city. You can ride the Powell-Mason or Powell-Hyde lines, which take you through some of the city's most iconic neighborhoods, including Nob Hill and Chinatown.\n",
      "\n",
      "In addition to these top three activities, San Francisco offers a wealth of other exciting experiences, such as visiting the vibrant Mission District, touring the former prison on Alcatraz Island, and exploring the diverse neighborhoods like Chinatown and North Beach. The city is renowned for its rich history, stunning architecture, and diverse cultural offerings, making it a truly captivating destination for visitors.\n",
      "\n",
      "I hope this synthesized response provides you with a comprehensive and accurate overview of some of the most fun and engaging activities to enjoy in San Francisco. Please let me know if you have any other questions!\n",
      "Input Token Usage: 6217, Output Token Usage: 2231\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        setTimeout(function() {\n",
       "            var nbb_cell_id = 14;\n",
       "            var nbb_formatted_code = \"async def main():\\n    \\\"\\\"\\\"Run the main loop of the MOA process.\\\"\\\"\\\"\\n    user_prompt = \\\"What are 3 fun things to do in SF?\\\"\\n\\n    messages = [{\\\"role\\\": \\\"user\\\", \\\"content\\\": [{\\\"text\\\": f\\\"User Query: {user_prompt}\\\"}]}]\\n\\n    input_token_usage = 0\\n    output_token_usage = 0\\n\\n    # Invoke Layer-1 of MoA\\n    results = await asyncio.gather(\\n        *[\\n            run_llms(\\n                modelId=reference_model[\\\"modelId\\\"],\\n                inference_params=reference_model[\\\"inference_params\\\"],\\n                maxTokens=reference_model[\\\"maxTokens\\\"],\\n                messages=messages,\\n                prev_response=None,\\n            )\\n            for reference_model in reference_models\\n        ]\\n    )\\n\\n    input_token_usage += sum([element[1] for element in results])\\n    output_token_usage += sum([element[2] for element in results])\\n    # Invoke Layer-2, Layer-3, .... Layer-(N-1) of MoA\\n    for _ in range(1, layers - 1):\\n        results = await asyncio.gather(\\n            *[\\n                run_llms(\\n                    modelId=reference_model[\\\"modelId\\\"],\\n                    inference_params=reference_model[\\\"inference_params\\\"],\\n                    maxTokens=reference_model[\\\"maxTokens\\\"],\\n                    messages=messages,\\n                    prev_response=results,\\n                )\\n                for reference_model in reference_models\\n            ]\\n        )\\n        input_token_usage += sum([element[1] for element in results])\\n        output_token_usage += sum([element[2] for element in results])\\n\\n    bedrock = Session().client(\\n        service_name=\\\"bedrock-runtime\\\",\\n    )\\n\\n    # Invoke the Aggregator model\\n    response = backoff_mechanism(\\n        func=invoke_model,\\n        modelId=aggregator_model[\\\"modelId\\\"],\\n        inference_params=aggregator_model[\\\"inference_params\\\"],\\n        maxTokens=aggregator_model[\\\"maxTokens\\\"],\\n        messages=messages,\\n        prev_response=results,\\n    )\\n\\n    input_token_usage += response[1]\\n    output_token_usage += response[2]\\n\\n    print(f\\\"Final Response:\\\\n\\\\n{response[0]}\\\")\\n    print(\\n        f\\\"Input Token Usage: {input_token_usage}, Output Token Usage: {output_token_usage}\\\"\\n    )\\n\\n\\nawait main()\";\n",
       "            var nbb_cells = Jupyter.notebook.get_cells();\n",
       "            for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                    nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                    break;\n",
       "                }\n",
       "            }\n",
       "        }, 500);\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async def main():\n",
    "    \"\"\"\n",
    "    Run the main loop of the MOA process.\n",
    "\n",
    "    This function runs the main loop of the MOA (Mixture of Annotators) process. \n",
    "    It takes a user prompt as input, initializes the messages list, and then invokes multiple layers of the \n",
    "    MOA process. Each layer involves running multiple language models in parallel and aggregating their responses \n",
    "    using an aggregator model.\n",
    "\n",
    "    The function keeps track of the input and output token usage across all layers and models. \n",
    "    Finally, it prints the final response from the aggregator model and the total input and output token usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = \"What are 3 fun things to do in SF?\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": f\"User Query: {user_prompt}\"}]}]\n",
    "\n",
    "    input_token_usage = 0\n",
    "    output_token_usage = 0\n",
    "\n",
    "    # Invoke Layer-1 of MoA\n",
    "    results = await asyncio.gather(\n",
    "        *[\n",
    "            run_llms(\n",
    "                modelId=reference_model[\"modelId\"],\n",
    "                inference_params=reference_model[\"inference_params\"],\n",
    "                maxTokens=reference_model[\"maxTokens\"],\n",
    "                messages=messages,\n",
    "                prev_response=None,\n",
    "            )\n",
    "            for reference_model in reference_models\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    input_token_usage += sum([element[1] for element in results])\n",
    "    output_token_usage += sum([element[2] for element in results])\n",
    "    # Invoke Layer-2, Layer-3, .... Layer-(N-1) of MoA\n",
    "    for _ in range(1, layers - 1):\n",
    "        results = await asyncio.gather(\n",
    "            *[\n",
    "                run_llms(\n",
    "                    modelId=reference_model[\"modelId\"],\n",
    "                    inference_params=reference_model[\"inference_params\"],\n",
    "                    maxTokens=reference_model[\"maxTokens\"],\n",
    "                    messages=messages,\n",
    "                    prev_response=results,\n",
    "                )\n",
    "                for reference_model in reference_models\n",
    "            ]\n",
    "        )\n",
    "        input_token_usage += sum([element[1] for element in results])\n",
    "        output_token_usage += sum([element[2] for element in results])\n",
    "\n",
    "    bedrock = Session().client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "    )\n",
    "\n",
    "    # Invoke the Aggregator model\n",
    "    response = backoff_mechanism(\n",
    "        func=invoke_model,\n",
    "        modelId=aggregator_model[\"modelId\"],\n",
    "        inference_params=aggregator_model[\"inference_params\"],\n",
    "        maxTokens=aggregator_model[\"maxTokens\"],\n",
    "        messages=messages,\n",
    "        prev_response=results,\n",
    "    )\n",
    "\n",
    "    input_token_usage += response[1]\n",
    "    output_token_usage += response[2]\n",
    "\n",
    "    print(f\"Final Response:\\n\\n{response[0]}\")\n",
    "    print(\n",
    "        f\"Input Token Usage: {input_token_usage}, Output Token Usage: {output_token_usage}\"\n",
    "    )\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be6e253",
   "metadata": {},
   "source": [
    "## Evaluation - AlpacaEval 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3d29a",
   "metadata": {},
   "source": [
    "AlpacaEval in an LLM-based automatic evaluation that is fast, cheap, replicable, and validated against 20K human annotations. We have utilized AlpacaEval 2.0 to evaluate MoA implementation. AlpacaEval 2.0 contains 805 instructions representative of real use cases. MoA's [response](/2.projects/mixture-of-agents/outputs/anthropic.claude-3-haiku-20240307-v1_0-moa-round-11.json) is directly compared against that of the [Anthropic Claude Sonnet 3.5](/2.projects/mixture-of-agents/alpaca_eval/results/claude-3-5-sonnet-20240620/model_outputs.json), with a GPT-4-based evaluator determining the likelihood of preferring the evaluated model’s response. To ensure fairness, the evaluation employs length-controlled (LC) win rates, effectively neutralizing length bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f5710",
   "metadata": {},
   "source": [
    "AlpacaEval 2.0 with length-controlled win-rates ([paper](/2.projects/mixture-of-agents/alpaca_eval)) has a spearman correlation of 0.98 with ChatBot Arena while costing less than <b>$10</b> of OpenAI credits run and running in less than 3 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94935994",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = datasets.load_dataset(\n",
    "    \"tatsu-lab/alpaca_eval\", \"alpaca_eval_gpt4_baseline\", trust_remote_code=True\n",
    ")[\"eval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf715c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = eval_set.remove_columns([\"output\", \"generator\"])\n",
    "eval_set = eval_set.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def invoke_moa(\n",
    "    item, reference_models, aggregator_model, layers, experimentation_round, debug=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Invoke the Mixture of Aggregators (MoA) model for a given item and reference models.\n",
    "\n",
    "    Args:\n",
    "        item (dict): A dictionary containing the user's instruction.\n",
    "        reference_models (list): A list of dictionaries containing the reference model IDs, inference parameters, \n",
    "            and maximum tokens.\n",
    "        aggregator_model (dict): A dictionary containing the aggregator model ID, inference parameters, \n",
    "            and maximum tokens.\n",
    "        layers (int): The number of layers to use in the MoA model.\n",
    "        experimentation_round (int): The current experimentation round.\n",
    "        debug (bool, optional): Whether to enable debug mode. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two dictionaries:\n",
    "            1. The first dictionary contains the original item, the output from the MoA model, and the generator ID.\n",
    "            2. The second dictionary contains the original item, the output from the MoA model, \n",
    "                the generator ID, the input token usage, the output token usage, and the total time taken.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [{\"text\": f\"User Query: {item['instruction']}\"}]}\n",
    "    ]\n",
    "\n",
    "    input_token_usage = 0\n",
    "    output_token_usage = 0\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    results = await asyncio.gather(\n",
    "        *[\n",
    "            run_llms(\n",
    "                modelId=reference_model[\"modelId\"],\n",
    "                inference_params=reference_model[\"inference_params\"],\n",
    "                maxTokens=reference_model[\"maxTokens\"],\n",
    "                messages=messages,\n",
    "                prev_response=None,\n",
    "                debug=debug,\n",
    "            )\n",
    "            for reference_model in reference_models\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    input_token_usage += sum([element[1] for element in results])\n",
    "    output_token_usage += sum([element[2] for element in results])\n",
    "\n",
    "    for _ in range(1, layers - 1):\n",
    "        results = await asyncio.gather(\n",
    "            *[\n",
    "                run_llms(\n",
    "                    modelId=reference_model[\"modelId\"],\n",
    "                    inference_params=reference_model[\"inference_params\"],\n",
    "                    maxTokens=reference_model[\"maxTokens\"],\n",
    "                    messages=messages,\n",
    "                    prev_response=results,\n",
    "                    debug=debug,\n",
    "                )\n",
    "                for reference_model in reference_models\n",
    "            ]\n",
    "        )\n",
    "        input_token_usage += sum([element[1] for element in results])\n",
    "        output_token_usage += sum([element[2] for element in results])\n",
    "\n",
    "    bedrock = Session().client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "    )\n",
    "    response = backoff_mechanism(\n",
    "        func=invoke_model,\n",
    "        modelId=aggregator_model[\"modelId\"],\n",
    "        inference_params=aggregator_model[\"inference_params\"],\n",
    "        maxTokens=aggregator_model[\"maxTokens\"],\n",
    "        messages=messages,\n",
    "        prev_response=results,\n",
    "        debug=debug,\n",
    "    )\n",
    "\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    input_token_usage += response[1]\n",
    "    output_token_usage += response[2]\n",
    "\n",
    "    return {\n",
    "        **item,\n",
    "        **{\n",
    "            \"output\": response[0],\n",
    "            \"generator\": aggregator_model[\"modelId\"]\n",
    "            + str(layers)\n",
    "            + str(experimentation_round)\n",
    "            + \"-moa\",\n",
    "        },\n",
    "    }, {\n",
    "        **item,\n",
    "        **{\n",
    "            \"output\": response[0],\n",
    "            \"generator\": aggregator_model[\"modelId\"]\n",
    "            + str(layers)\n",
    "            + str(experimentation_round)\n",
    "            + \"-moa\",\n",
    "            \"input_token_usage\": input_token_usage,\n",
    "            \"output_token_usage\": output_token_usage,\n",
    "            \"total_time\": total_time,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901972e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await invoke_moa(\n",
    "    eval_set[0],\n",
    "    reference_models=reference_models,\n",
    "    aggregator_model=aggregator_model,\n",
    "    layers=2,\n",
    "    experimentation_round=2,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eb74e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What are the names of some famous actors that started their careers on Broadway?',\n",
       " 'dataset': 'helpful_base',\n",
       " 'output': 'Here is a synthesis of the key information provided in the previous responses about famous actors who started their careers on Broadway:\\n\\nMany acclaimed actors and actresses began their professional acting careers on the Broadway stage before finding success in film and television. Some notable examples include:\\n\\n- Al Pacino - Got his start in Broadway productions like \"The Indian Wants the Bronx\" and \"Does a Tiger Wear a Necktie?\" in the 1960s.\\n\\n- Meryl Streep - Appeared in Broadway shows such as \"Trelawny of the \\'Wells\\'\" and \"Happy End\" in the 1970s.\\n\\n- Robert De Niro - Made his Broadway debut in the 1968 play \"The Wedding Party\" before transitioning to film.\\n\\n- Denzel Washington - Starred in Broadway productions like \"Checkmates\" and \"Julius Caesar\" early in his career.\\n\\n- Viola Davis - Had an extensive theater background, including roles in Broadway plays like \"King Hedley II\" and \"Fences.\"\\n\\n- Kevin Kline, Glenn Close, and Christopher Walken also launched their acting careers on the New York stage before becoming major film and TV stars.\\n\\nOther examples include James Dean, Liza Minnelli, Anne Hathaway, Hugh Jackman, and Whoopi Goldberg, among others. Many of these performers honed their craft on Broadway before finding widespread fame and acclaim in Hollywood.',\n",
       " 'generator': 'anthropic.claude-3-haiku-20240307-v1:022-moa'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        setTimeout(function() {\n",
       "            var nbb_cell_id = 19;\n",
       "            var nbb_formatted_code = \"response[0]\";\n",
       "            var nbb_cells = Jupyter.notebook.get_cells();\n",
       "            for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                    nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                    break;\n",
       "                }\n",
       "            }\n",
       "        }, 500);\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c309db55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What are the names of some famous actors that started their careers on Broadway?',\n",
       " 'dataset': 'helpful_base',\n",
       " 'output': 'Here is a synthesis of the key information provided in the previous responses about famous actors who started their careers on Broadway:\\n\\nMany acclaimed actors and actresses began their professional acting careers on the Broadway stage before finding success in film and television. Some notable examples include:\\n\\n- Al Pacino - Got his start in Broadway productions like \"The Indian Wants the Bronx\" and \"Does a Tiger Wear a Necktie?\" in the 1960s.\\n\\n- Meryl Streep - Appeared in Broadway shows such as \"Trelawny of the \\'Wells\\'\" and \"Happy End\" in the 1970s.\\n\\n- Robert De Niro - Made his Broadway debut in the 1968 play \"The Wedding Party\" before transitioning to film.\\n\\n- Denzel Washington - Starred in Broadway productions like \"Checkmates\" and \"Julius Caesar\" early in his career.\\n\\n- Viola Davis - Had an extensive theater background, including roles in Broadway plays like \"King Hedley II\" and \"Fences.\"\\n\\n- Kevin Kline, Glenn Close, and Christopher Walken also launched their acting careers on the New York stage before becoming major film and TV stars.\\n\\nOther examples include James Dean, Liza Minnelli, Anne Hathaway, Hugh Jackman, and Whoopi Goldberg, among others. Many of these performers honed their craft on Broadway before finding widespread fame and acclaim in Hollywood.',\n",
       " 'generator': 'anthropic.claude-3-haiku-20240307-v1:022-moa',\n",
       " 'input_token_usage': 1512,\n",
       " 'output_token_usage': 1484,\n",
       " 'total_time': 11.312489481992088}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        setTimeout(function() {\n",
       "            var nbb_cell_id = 20;\n",
       "            var nbb_formatted_code = \"response[1]\";\n",
       "            var nbb_cells = Jupyter.notebook.get_cells();\n",
       "            for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                    nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                    break;\n",
       "                }\n",
       "            }\n",
       "        }, 500);\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1ab1481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0!\n",
      "Completed 5!\n",
      "Completed 10!\n",
      "Completed 15!\n",
      "Completed 20!\n",
      "Completed 25!\n",
      "Completed 30!\n",
      "Completed 35!\n",
      "Completed 40!\n",
      "Completed 45!\n",
      "Completed 50!\n",
      "Completed 55!\n",
      "Completed 60!\n",
      "Completed 65!\n",
      "Completed 70!\n",
      "Completed 75!\n",
      "Completed 80!\n",
      "Completed 85!\n",
      "Completed 90!\n",
      "Completed 95!\n",
      "Completed 100!\n",
      "Completed 105!\n",
      "Completed 110!\n",
      "Completed 115!\n",
      "Completed 120!\n",
      "Completed 125!\n",
      "Completed 130!\n",
      "Completed 135!\n",
      "Completed 140!\n",
      "Completed 145!\n",
      "Completed 150!\n",
      "Completed 155!\n",
      "Completed 160!\n",
      "Completed 165!\n",
      "Completed 170!\n",
      "Completed 175!\n",
      "Completed 180!\n",
      "Completed 185!\n",
      "Completed 190!\n",
      "Completed 195!\n",
      "Completed 200!\n",
      "Completed 205!\n",
      "Completed 210!\n",
      "Completed 215!\n",
      "Completed 220!\n",
      "Completed 225!\n",
      "Completed 230!\n",
      "Completed 235!\n",
      "Completed 240!\n",
      "Completed 245!\n",
      "Completed 250!\n",
      "Completed 255!\n",
      "Completed 260!\n",
      "Completed 265!\n",
      "Completed 270!\n",
      "Completed 275!\n",
      "Completed 280!\n",
      "Completed 285!\n",
      "Completed 290!\n",
      "Completed 295!\n",
      "Completed 300!\n",
      "Completed 305!\n",
      "Completed 310!\n",
      "Completed 315!\n",
      "Completed 320!\n",
      "Completed 325!\n",
      "Completed 330!\n",
      "Completed 335!\n",
      "Completed 340!\n",
      "Completed 345!\n",
      "Completed 350!\n",
      "Completed 355!\n",
      "Completed 360!\n",
      "Completed 365!\n",
      "Completed 370!\n",
      "Completed 375!\n",
      "Completed 380!\n",
      "Completed 385!\n",
      "Completed 390!\n",
      "Completed 395!\n",
      "Completed 400!\n",
      "Completed 405!\n",
      "Completed 410!\n",
      "Completed 415!\n",
      "Completed 420!\n",
      "Completed 425!\n",
      "Completed 430!\n",
      "Completed 435!\n",
      "Completed 440!\n",
      "Completed 445!\n",
      "Completed 450!\n",
      "Completed 455!\n",
      "Completed 460!\n",
      "Completed 465!\n",
      "Completed 470!\n",
      "Completed 475!\n",
      "Completed 480!\n",
      "Completed 485!\n",
      "Completed 490!\n",
      "Completed 495!\n",
      "Completed 500!\n",
      "Completed 505!\n",
      "Completed 510!\n",
      "Completed 515!\n",
      "Completed 520!\n",
      "Completed 525!\n",
      "Completed 530!\n",
      "Completed 535!\n",
      "Completed 540!\n",
      "Completed 545!\n",
      "Completed 550!\n",
      "Completed 555!\n",
      "Completed 560!\n",
      "Completed 565!\n",
      "Completed 570!\n",
      "Completed 575!\n",
      "Completed 580!\n",
      "Completed 585!\n",
      "Completed 590!\n",
      "Completed 595!\n",
      "Completed 600!\n",
      "Completed 605!\n",
      "Completed 610!\n",
      "Completed 615!\n",
      "Completed 620!\n",
      "Completed 625!\n",
      "Completed 630!\n",
      "Completed 635!\n",
      "Completed 640!\n",
      "Completed 645!\n",
      "Completed 650!\n",
      "Completed 655!\n",
      "Completed 660!\n",
      "Completed 665!\n",
      "Completed 670!\n",
      "Completed 675!\n",
      "Completed 680!\n",
      "Completed 685!\n",
      "Completed 690!\n",
      "Completed 695!\n",
      "Completed 700!\n",
      "Completed 705!\n",
      "Completed 710!\n",
      "Completed 715!\n",
      "Completed 720!\n",
      "Completed 725!\n",
      "Completed 730!\n",
      "Completed 735!\n",
      "Completed 740!\n",
      "Completed 745!\n",
      "Completed 750!\n",
      "Completed 755!\n",
      "Completed 760!\n",
      "Completed 765!\n",
      "Completed 770!\n",
      "Completed 775!\n",
      "Completed 780!\n",
      "Completed 785!\n",
      "Completed 790!\n",
      "Completed 795!\n",
      "Completed 800!\n",
      "Total time: 7040.109539813013 seconds\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        setTimeout(function() {\n",
       "            var nbb_cell_id = 21;\n",
       "            var nbb_formatted_code = \"experimentation_round = 12\\n\\n\\nasync def eval_moa(experimentation_round=experimentation_round):\\n\\n    new_eval_set = list()\\n    extended_eval_set = list()\\n    start_time = time.perf_counter()\\n\\n    input_token_usage = 0\\n    output_token_usage = 0\\n\\n    for idx in range(0, len(eval_set), 5):\\n        results = await asyncio.gather(\\n            *[\\n                invoke_moa(\\n                    item,\\n                    reference_models=reference_models,\\n                    aggregator_model=aggregator_model,\\n                    layers=layers,\\n                    experimentation_round=experimentation_round,\\n                    debug=False,\\n                )\\n                for item in eval_set[idx : idx + 5]\\n            ]\\n        )\\n\\n        new_eval_set_items = [result[0] for result in results]\\n        extended_eval_set_items = [result[1] for result in results]\\n\\n        # Wait for the futures to complete and collect the results\\n        new_eval_set.extend(new_eval_set_items)\\n        extended_eval_set.extend(extended_eval_set_items)\\n        print(f\\\"Completed {idx}!\\\")\\n\\n    total_time = time.perf_counter() - start_time\\n    print(f\\\"Total time: {total_time} seconds\\\")\\n\\n    with open(\\n        f\\\"outputs/{aggregator_model['modelId']}-moa-extended-eval-set-round-{experimentation_round}.json\\\",\\n        \\\"w\\\",\\n    ) as f:\\n        json.dump(list(extended_eval_set), f, indent=2)\\n\\n    with open(\\n        f\\\"outputs/{aggregator_model['modelId']}-moa-round-{experimentation_round}.json\\\",\\n        \\\"w\\\",\\n    ) as f:\\n        json.dump(list(new_eval_set), f, indent=2)\\n\\n\\nawait eval_moa()\";\n",
       "            var nbb_cells = Jupyter.notebook.get_cells();\n",
       "            for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                    nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                    break;\n",
       "                }\n",
       "            }\n",
       "        }, 500);\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experimentation_round = 11 # for tracking different implementations of MoA architecture. \n",
    "\n",
    "\n",
    "async def eval_moa(experimentation_round=experimentation_round):\n",
    "    \"\"\"\n",
    "    Evaluate the Mixture of Aggregators (MoA) model on a given evaluation set.\n",
    "\n",
    "    Args:\n",
    "        experimentation_round (int, optional): The current experimentation round. Defaults to the value of \n",
    "        `experimentation_round`.\n",
    "\n",
    "    This function evaluates the MoA model on the `eval_set` by invoking the `invoke_moa` function for each item \n",
    "    in the set. It processes the items in batches of 5 to improve efficiency. The function collects the results \n",
    "    from `invoke_moa` and stores them in two lists: `new_eval_set` and `extended_eval_set`.\n",
    "\n",
    "    The `new_eval_set` contains the original item dictionary with the output from the MoA model and the generator ID.\n",
    "    The `extended_eval_set` contains additional information such as input token usage, output token usage, \n",
    "    and total time taken.\n",
    "\n",
    "    After processing all items, the function writes the `extended_eval_set` and `new_eval_set` to JSON files with \n",
    "    filenames based on the aggregator model ID and the experimentation round.\n",
    "\n",
    "    The function also prints the total time taken to evaluate the MoA model on the evaluation set.\n",
    "    \"\"\"\n",
    "    new_eval_set = list()\n",
    "    extended_eval_set = list()\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    input_token_usage = 0\n",
    "    output_token_usage = 0\n",
    "\n",
    "    for idx in range(0, len(eval_set), 5):\n",
    "        results = await asyncio.gather(\n",
    "            *[\n",
    "                invoke_moa(\n",
    "                    item,\n",
    "                    reference_models=reference_models,\n",
    "                    aggregator_model=aggregator_model,\n",
    "                    layers=layers,\n",
    "                    experimentation_round=experimentation_round,\n",
    "                    debug=False,\n",
    "                )\n",
    "                for item in eval_set[idx : idx + 5]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        new_eval_set_items = [result[0] for result in results]\n",
    "        extended_eval_set_items = [result[1] for result in results]\n",
    "\n",
    "        # Wait for the futures to complete and collect the results\n",
    "        new_eval_set.extend(new_eval_set_items)\n",
    "        extended_eval_set.extend(extended_eval_set_items)\n",
    "        print(f\"Completed {idx}!\")\n",
    "\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    print(f\"Total time: {total_time} seconds\")\n",
    "\n",
    "    with open(\n",
    "        f\"outputs/{aggregator_model['modelId']}-moa-extended-eval-set-round-{experimentation_round}.json\",\n",
    "        \"w\",\n",
    "    ) as f:\n",
    "        json.dump(list(extended_eval_set), f, indent=2)\n",
    "\n",
    "    with open(\n",
    "        f\"outputs/{aggregator_model['modelId']}-moa-round-{experimentation_round}.json\",\n",
    "        \"w\",\n",
    "    ) as f:\n",
    "        json.dump(list(new_eval_set), f, indent=2)\n",
    "\n",
    "\n",
    "await eval_moa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5bf69d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 805 outputs.\n",
      "All outputs are of length greater than 0.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        setTimeout(function() {\n",
       "            var nbb_cell_id = 22;\n",
       "            var nbb_formatted_code = \"with open(\\n    f\\\"outputs/{aggregator_model['modelId']}-moa-round-{experimentation_round}.json\\\"\\n) as f:\\n    outputs = json.loads(f.read())\\n\\ntry:\\n    assert len(outputs) == len(eval_set)\\n    print(f\\\"We have a total of {len(outputs)} outputs.\\\")\\n\\n    for output in outputs:\\n        assert len(output[\\\"output\\\"]) != 0\\n\\n    print(f\\\"All outputs are of length greater than 0.\\\")\\n\\nexcept:\\n    assert len(outputs) == len(eval_set)\\n    print(\\\"Not all outputs are of length greater than 0.\\\")\";\n",
       "            var nbb_cells = Jupyter.notebook.get_cells();\n",
       "            for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                    nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                    break;\n",
       "                }\n",
       "            }\n",
       "        }, 500);\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate if all the responses for the dataset was generated\n",
    "with open(\n",
    "    f\"outputs/{aggregator_model['modelId']}-moa-round-{experimentation_round}.json\"\n",
    ") as f:\n",
    "    outputs = json.loads(f.read())\n",
    "\n",
    "try:\n",
    "    assert len(outputs) == len(eval_set)\n",
    "    print(f\"We have a total of {len(outputs)} outputs.\")\n",
    "\n",
    "    for output in outputs:\n",
    "        assert len(output[\"output\"]) != 0\n",
    "\n",
    "    print(f\"All outputs are of length greater than 0.\")\n",
    "\n",
    "except:\n",
    "    assert len(outputs) == len(eval_set)\n",
    "    print(\"Not all outputs are of length greater than 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a90b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpacha Eval evaluation\n",
    "!alpaca_eval --model_outputs outputs/anthropic.claude-3-haiku-20240307-v1:0-moa-round-11.json --reference_outputs alpaca_eval/results/claude-3-5-sonnet-20240620/model_outputs.json --output_path leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ec673",
   "metadata": {},
   "source": [
    "## Evaluation - Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690b16a",
   "metadata": {},
   "source": [
    "In this section, we will calculate the total cost for running inference using MoA on the dataset and compare it with the cost of Anthropic Claude 3.5 Sonnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f8718",
   "metadata": {},
   "source": [
    "| Models            | Price per 1,000 input tokens | Price per 1,000 output tokens |\n",
    "| :---------------- | :------: | ----: |\n",
    "| Claude 3 Haiku    |   0.00025   | 0.00125 |\n",
    "| Mixtral 8*7B      |   0.00045   | 0.0007 |\n",
    "| Llama 3.2 Instruct (3B) |  0.00015   | 0.00015 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the output file generated after running MoA. The file tracks input and output token usage. \n",
    "with open(\n",
    "    f\"outputs/{aggregator_model['modelId']}-moa-extended-eval-set-round-{experimentation_round}.json\"\n",
    ") as f:\n",
    "    evaluation_output = pd.DataFrame(json.loads(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e112323",
   "metadata": {},
   "outputs": [],
   "source": [
    "moa_input_token_usage = int(evaluation_output.input_token_usage.sum())\n",
    "moa_output_token_usage = int(evaluation_output.output_token_usage.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_price_moa = (moa_input_token_usage / 1000) * 0.00045 + (\n",
    "    moa_output_token_usage / 1000\n",
    ") * 0.00125"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edaeba6",
   "metadata": {},
   "source": [
    "| Models            | Price per 1,000 input tokens | Price per 1,000 output tokens |\n",
    "| :---------------- | :------: | ----: |\n",
    "| Claude 3.5 Sonnet |   0.003   | 0.015 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9639074",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_token_usage = sum(\n",
    "    evaluation_output.instruction.apply(lambda prompt: client.count_tokens(prompt))\n",
    ")\n",
    "model_output_token_usage = sum(\n",
    "    evaluation_output.output.apply(lambda prompt: client.count_tokens(prompt))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2edccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_price_sonnet = (model_input_token_usage / 1000) * 0.003 + (\n",
    "    model_output_token_usage / 1000\n",
    ") * 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c42b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Total price of running MOA inference on AlpacaEval 2.0 is ${total_price_moa} and total price of running Anthropic Sonnet 3.5 inference is ${total_price_sonnet}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9766ccf",
   "metadata": {},
   "source": [
    "## Evaluation - Latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28871820",
   "metadata": {},
   "source": [
    "In this section, we will first calculate the total time for running inference using Anthropic Claude 3.5 Sonnet on the dataset and compare it with the MoA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_latency = list()\n",
    "for prompt in evaluation_output[:200].instruction:\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": f\"User Query: {prompt}\"}]}]\n",
    "    start_time = time.perf_counter()\n",
    "    backoff_mechanism(\n",
    "        func=invoke_model,\n",
    "        modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        inference_params={\"temperature\": 0.5, \"topP\": 1.0, \"top_k\": 250},\n",
    "        maxTokens=512,\n",
    "        prev_response=\"\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    sonnet_latency.append(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(\n",
    "    [1, 2],\n",
    "    [\n",
    "        sum(sonnet_latency) / len(sonnet_latency),\n",
    "        sum(evaluation_output[:200].total_time) / len(evaluation_output[:200]),\n",
    "    ],\n",
    "    tick_label=[\"Single Model latency\", \"MoA latency\"],\n",
    "    align=\"center\",\n",
    ")\n",
    "ax.set_xlabel(\"Latency in seconds\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

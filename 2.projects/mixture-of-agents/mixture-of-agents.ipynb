{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b75185d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34460bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 1;\n            var nbb_formatted_code = \"%load_ext nb_black\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d790e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: alpaca-eval in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.6.5)\n",
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.0.1)\n",
      "Requirement already satisfied: anthropic-bedrock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.8.0)\n",
      "Requirement already satisfied: python-dotenv in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alpaca-eval->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: openai>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alpaca-eval->-r requirements.txt (line 1)) (1.51.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alpaca-eval->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alpaca-eval->-r requirements.txt (line 1)) (0.8.0)\n",
      "Requirement already satisfied: fire in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alpaca-eval->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alpaca-eval->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alpaca-eval->-r requirements.txt (line 1)) (0.25.1)\n",
      "Requirement already satisfied: patsy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alpaca-eval->-r requirements.txt (line 1)) (0.5.6)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alpaca-eval->-r requirements.txt (line 1)) (1.5.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->-r requirements.txt (line 2)) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (3.9.5)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic-bedrock->-r requirements.txt (line 3)) (4.4.0)\n",
      "Requirement already satisfied: boto3>=1.28.57 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic-bedrock->-r requirements.txt (line 3)) (1.35.16)\n",
      "Requirement already satisfied: botocore>=1.31.57 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic-bedrock->-r requirements.txt (line 3)) (1.35.16)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic-bedrock->-r requirements.txt (line 3)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic-bedrock->-r requirements.txt (line 3)) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic-bedrock->-r requirements.txt (line 3)) (2.9.1)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic-bedrock->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic-bedrock->-r requirements.txt (line 3)) (0.20.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic-bedrock->-r requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic-bedrock->-r requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic-bedrock->-r requirements.txt (line 3)) (1.2.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.28.57->anthropic-bedrock->-r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.28.57->anthropic-bedrock->-r requirements.txt (line 3)) (0.10.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.31.57->anthropic-bedrock->-r requirements.txt (line 3)) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.31.57->anthropic-bedrock->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic-bedrock->-r requirements.txt (line 3)) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic-bedrock->-r requirements.txt (line 3)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic-bedrock->-r requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.5.0->alpaca-eval->-r requirements.txt (line 1)) (0.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic-bedrock->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic-bedrock->-r requirements.txt (line 3)) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tiktoken>=0.3.2->alpaca-eval->-r requirements.txt (line 1)) (2024.7.24)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fire->alpaca-eval->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->alpaca-eval->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->alpaca-eval->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from patsy->alpaca-eval->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->alpaca-eval->-r requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->alpaca-eval->-r requirements.txt (line 1)) (3.5.0)\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 2;\n            var nbb_formatted_code = \"!pip install -r requirements.txt\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ed2ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 3;\n            var nbb_formatted_code = \"import time\\nimport asyncio\\nimport concurrent.futures\\nimport random\\nimport os\\nfrom functools import partial\\nimport json\\n\\nimport pandas as pd\\nimport datasets\\nimport matplotlib.pyplot as plt\\n\\nfrom boto3.session import Session\\nfrom botocore.exceptions import ClientError\\nfrom anthropic_bedrock import AnthropicBedrock\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import random\n",
    "import os\n",
    "from functools import partial\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from boto3.session import Session\n",
    "from botocore.exceptions import ClientError\n",
    "from anthropic_bedrock import AnthropicBedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc6daf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 4;\n            var nbb_formatted_code = \"client = AnthropicBedrock()\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = AnthropicBedrock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac39e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 5;\n            var nbb_formatted_code = \"os.environ[\\\"OPENAI_API_KEY\\\"] = (\\n    \\\"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\\\"\\n)\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "   <OPEN-AI_API-KEY>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad82368",
   "metadata": {},
   "source": [
    "## Mixture of Agents Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055f5c3",
   "metadata": {},
   "source": [
    "In this scenario, we will be employing a two-layered Mixture of Experts (MoE) architecture. The initial two layers comprise proposers, each consisting of three Large Language Models (LLMs). These LLMs have the capability to operate with varying inference parameters, enabling them to propose a diverse range of responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519fa612",
   "metadata": {},
   "source": [
    "![architecture](architecture-simple.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc5ce96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 6;\n            var nbb_formatted_code = \"reference_models = [\\n    {\\n        \\\"modelId\\\": \\\"anthropic.claude-3-haiku-20240307-v1:0\\\",\\n        \\\"inference_params\\\": {\\\"temperature\\\": 0.5, \\\"topP\\\": 1.0, \\\"top_k\\\": 250},\\n        \\\"maxTokens\\\": 512,\\n    },\\n    {\\n        \\\"modelId\\\": \\\"mistral.mixtral-8x7b-instruct-v0:1\\\",\\n        \\\"inference_params\\\": {\\\"temperature\\\": 0.7, \\\"topP\\\": 1.0},\\n        \\\"maxTokens\\\": 512,\\n    },\\n    {\\n        \\\"modelId\\\": \\\"us.meta.llama3-2-3b-instruct-v1:0\\\",\\n        \\\"inference_params\\\": {\\\"temperature\\\": 0.5, \\\"topP\\\": 0.9},\\n        \\\"maxTokens\\\": 512,\\n    },\\n]\\n\\naggregator_model = {\\n    \\\"modelId\\\": \\\"anthropic.claude-3-haiku-20240307-v1:0\\\",\\n    \\\"inference_params\\\": {\\\"temperature\\\": 0.0, \\\"topP\\\": 1.0, \\\"top_k\\\": 250},\\n    \\\"maxTokens\\\": 512,\\n}\\n\\naggreagator_system_prompt = \\\"\\\"\\\"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability. Do not write in response that this was synthesised from previous responses\\n\\nResponses from models:\\\"\\\"\\\"\\n\\nlayers = 3\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reference_models = [\n",
    "    {\n",
    "        \"modelId\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        \"inference_params\": {\"temperature\": 0.5, \"topP\": 1.0, \"top_k\": 250},\n",
    "        \"maxTokens\": 512,\n",
    "    },\n",
    "    {\n",
    "        \"modelId\": \"mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "        \"inference_params\": {\"temperature\": 0.7, \"topP\": 1.0},\n",
    "        \"maxTokens\": 512,\n",
    "    },\n",
    "    {\n",
    "        \"modelId\": \"us.meta.llama3-2-3b-instruct-v1:0\",\n",
    "        \"inference_params\": {\"temperature\": 0.5, \"topP\": 0.9},\n",
    "        \"maxTokens\": 512,\n",
    "    },\n",
    "]\n",
    "\n",
    "aggregator_model = {\n",
    "    \"modelId\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"inference_params\": {\"temperature\": 0.0, \"topP\": 1.0, \"top_k\": 250},\n",
    "    \"maxTokens\": 512,\n",
    "}\n",
    "\n",
    "aggreagator_system_prompt = \"\"\"You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability. Do not write in response that this was synthesised from previous responses\n",
    "\n",
    "Responses from models:\"\"\"\n",
    "\n",
    "layers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a718c",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "534fa880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 7;\n            var nbb_formatted_code = \"def getFinalSystemPrompt(system_prompt, results):\\n    \\\"\\\"\\\"Construct a system prompt for layers 2+ that includes the previous responses to synthesize.\\\"\\\"\\\"\\n    for i, element in enumerate(results):\\n        if len(element[0]) == 0:\\n            return None\\n    return (\\n        system_prompt\\n        + \\\"\\\\n\\\"\\n        + \\\"\\\\n\\\".join(\\n            [\\n                f\\\"<Response_{i+1}> {str(element[0])} </Response_{i+1}> \\\\n\\\\n\\\"\\n                for i, element in enumerate(results)\\n            ]\\n        )\\n    )\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getFinalSystemPrompt(system_prompt, results):\n",
    "    \"\"\"Construct a system prompt for layers 2+ that includes the previous responses to synthesize.\"\"\"\n",
    "    for i, element in enumerate(results):\n",
    "        if len(element[0]) == 0:\n",
    "            return None\n",
    "    return (\n",
    "        system_prompt\n",
    "        + \"\\n\"\n",
    "        + \"\\n\".join(\n",
    "            [\n",
    "                f\"<Response_{i+1}> {str(element[0])} </Response_{i+1}> \\n\\n\"\n",
    "                for i, element in enumerate(results)\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5298e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability. Do not write in response that this was synthesised from previous responses\n",
      "\n",
      "Responses from models:\n",
      "<Response_1> Here are 3 fun things to do in San Francisco:\n",
      "\n",
      "1. Visit the Golden Gate Bridge - This iconic landmark offers stunning views of the city and the bay. You can walk or bike across the bridge for an up-close experience.\n",
      "\n",
      "2. Explore Fisherman's Wharf - This lively waterfront area is home to seafood restaurants, street performers, the Musée Mécanique arcade, and Pier 39 with its sea lion colony.\n",
      "\n",
      "3. Ride the Cable Cars - San Francisco's historic cable cars are a fun and unique way to get around the hilly city. You can ride the cable cars up and down the steep streets.\n",
      "\n",
      "Some other popular activities in San Francisco include visiting the colorful murals of the Mission District, touring Alcatraz Island, checking out the views from Twin Peaks, and exploring the shops and restaurants in neighborhoods like North Beach and the Castro. There are lots of fun and unique experiences to be had in this vibrant city. </Response_1> \n",
      "\n",
      "\n",
      "<Response_2>  Sure, I'd be happy to help! San Francisco is a city with no shortage of fun activities. Here are three fun things you might consider doing:\n",
      "\n",
      "1. Visit the Exploratorium: The Exploratorium is a hands-on science museum located on the Embarcadero. It features over 650 interactive exhibits that cover a wide range of topics, from physics and biology to art and psychology. You can play with optical illusions, create your own mini-tornado, or explore the world of microbes. It's a great place to learn and have fun at the same time.\n",
      "2. Take a Ferry to Sausalito: Sausalito is a charming town located across the bay from San Francisco. You can take a ferry from the Ferry Building or Pier 39 and enjoy the scenic views of the Bay and the Golden Gate Bridge. Once you arrive in Sausalito, you can explore the town's many art galleries, boutiques, and restaurants. You can also rent a bike and ride along the waterfront.\n",
      "3. Visit the California Academy of Sciences: The California Academy of Sciences is a natural history museum located in Golden Gate Park. It features an aquarium, a planetarium, a rainforest dome, and a natural history museum. You can learn about the diverse ecosystems of California, see penguins and other marine life, and take a journey through the stars in the planetarium. It's a great place to spend a day exploring and learning about the natural world.\n",
      "\n",
      "I hope these suggestions are helpful and that you have a great time in San Francisco! </Response_2> \n",
      "\n",
      "\n",
      "<Response_3> San Francisco is a vibrant city with endless options for entertainment and exploration. Here are 3 fun things to do in SF:\n",
      "\n",
      "1. **Explore Fisherman's Wharf and Pier 39**: This iconic waterfront district is known for its stunning views of the Golden Gate Bridge, sea lions at Pier 39, and a variety of seafood restaurants. You can also take a stroll along the pier, visit the Aquarium of the Bay, or ride the historic cable cars.\n",
      "\n",
      "2. **Take a stroll across the Golden Gate Bridge**: This iconic suspension bridge is a must-visit attraction in SF. You can walk, bike, or drive across the bridge for breathtaking views of the San Francisco Bay, Alcatraz Island, and the city skyline. If you're feeling adventurous, you can even take a guided tour or bike ride across the bridge.\n",
      "\n",
      "3. **Visit Alcatraz Island**: Once a notorious maximum-security prison, Alcatraz Island is now a popular tourist attraction. Take a ferry to the island and explore the former prison cells, listen to a guided audio tour, or take a self-guided tour of the island. You can also see the famous \"Cell Block C\" and learn about the island's infamous history.\n",
      "\n",
      "These are just a few of the many fun things to do in SF. Whether you're interested in history, nature, or entertainment, there's something for everyone in this vibrant city! </Response_3> \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 8;\n            var nbb_formatted_code = \"print(\\n    getFinalSystemPrompt(\\n        system_prompt=aggreagator_system_prompt,\\n        results=[\\n            (\\n                \\\"Here are 3 fun things to do in San Francisco:\\\\n\\\\n1. Visit the Golden Gate Bridge - This iconic landmark offers stunning views of the city and the bay. You can walk or bike across the bridge for an up-close experience.\\\\n\\\\n2. Explore Fisherman's Wharf - This lively waterfront area is home to seafood restaurants, street performers, the Mus\\u00e9e M\\u00e9canique arcade, and Pier 39 with its sea lion colony.\\\\n\\\\n3. Ride the Cable Cars - San Francisco's historic cable cars are a fun and unique way to get around the hilly city. You can ride the cable cars up and down the steep streets.\\\\n\\\\nSome other popular activities in San Francisco include visiting the colorful murals of the Mission District, touring Alcatraz Island, checking out the views from Twin Peaks, and exploring the shops and restaurants in neighborhoods like North Beach and the Castro. There are lots of fun and unique experiences to be had in this vibrant city.\\\",\\n                22,\\n                214,\\n            ),\\n            (\\n                \\\" Sure, I'd be happy to help! San Francisco is a city with no shortage of fun activities. Here are three fun things you might consider doing:\\\\n\\\\n1. Visit the Exploratorium: The Exploratorium is a hands-on science museum located on the Embarcadero. It features over 650 interactive exhibits that cover a wide range of topics, from physics and biology to art and psychology. You can play with optical illusions, create your own mini-tornado, or explore the world of microbes. It's a great place to learn and have fun at the same time.\\\\n2. Take a Ferry to Sausalito: Sausalito is a charming town located across the bay from San Francisco. You can take a ferry from the Ferry Building or Pier 39 and enjoy the scenic views of the Bay and the Golden Gate Bridge. Once you arrive in Sausalito, you can explore the town's many art galleries, boutiques, and restaurants. You can also rent a bike and ride along the waterfront.\\\\n3. Visit the California Academy of Sciences: The California Academy of Sciences is a natural history museum located in Golden Gate Park. It features an aquarium, a planetarium, a rainforest dome, and a natural history museum. You can learn about the diverse ecosystems of California, see penguins and other marine life, and take a journey through the stars in the planetarium. It's a great place to spend a day exploring and learning about the natural world.\\\\n\\\\nI hope these suggestions are helpful and that you have a great time in San Francisco!\\\",\\n                30,\\n                353,\\n            ),\\n            (\\n                \\\"San Francisco is a vibrant city with endless options for entertainment and exploration. Here are 3 fun things to do in SF:\\\\n\\\\n1. **Explore Fisherman's Wharf and Pier 39**: This iconic waterfront district is known for its stunning views of the Golden Gate Bridge, sea lions at Pier 39, and a variety of seafood restaurants. You can also take a stroll along the pier, visit the Aquarium of the Bay, or ride the historic cable cars.\\\\n\\\\n2. **Take a stroll across the Golden Gate Bridge**: This iconic suspension bridge is a must-visit attraction in SF. You can walk, bike, or drive across the bridge for breathtaking views of the San Francisco Bay, Alcatraz Island, and the city skyline. If you're feeling adventurous, you can even take a guided tour or bike ride across the bridge.\\\\n\\\\n3. **Visit Alcatraz Island**: Once a notorious maximum-security prison, Alcatraz Island is now a popular tourist attraction. Take a ferry to the island and explore the former prison cells, listen to a guided audio tour, or take a self-guided tour of the island. You can also see the famous \\\\\\\"Cell Block C\\\\\\\" and learn about the island's infamous history.\\\\n\\\\nThese are just a few of the many fun things to do in SF. Whether you're interested in history, nature, or entertainment, there's something for everyone in this vibrant city!\\\",\\n                49,\\n                284,\\n            ),\\n        ],\\n    )\\n)\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\n",
    "    getFinalSystemPrompt(\n",
    "        system_prompt=aggreagator_system_prompt,\n",
    "        results=[\n",
    "            (\n",
    "                \"Here are 3 fun things to do in San Francisco:\\n\\n1. Visit the Golden Gate Bridge - This iconic landmark offers stunning views of the city and the bay. You can walk or bike across the bridge for an up-close experience.\\n\\n2. Explore Fisherman's Wharf - This lively waterfront area is home to seafood restaurants, street performers, the Musée Mécanique arcade, and Pier 39 with its sea lion colony.\\n\\n3. Ride the Cable Cars - San Francisco's historic cable cars are a fun and unique way to get around the hilly city. You can ride the cable cars up and down the steep streets.\\n\\nSome other popular activities in San Francisco include visiting the colorful murals of the Mission District, touring Alcatraz Island, checking out the views from Twin Peaks, and exploring the shops and restaurants in neighborhoods like North Beach and the Castro. There are lots of fun and unique experiences to be had in this vibrant city.\",\n",
    "                22,\n",
    "                214,\n",
    "            ),\n",
    "            (\n",
    "                \" Sure, I'd be happy to help! San Francisco is a city with no shortage of fun activities. Here are three fun things you might consider doing:\\n\\n1. Visit the Exploratorium: The Exploratorium is a hands-on science museum located on the Embarcadero. It features over 650 interactive exhibits that cover a wide range of topics, from physics and biology to art and psychology. You can play with optical illusions, create your own mini-tornado, or explore the world of microbes. It's a great place to learn and have fun at the same time.\\n2. Take a Ferry to Sausalito: Sausalito is a charming town located across the bay from San Francisco. You can take a ferry from the Ferry Building or Pier 39 and enjoy the scenic views of the Bay and the Golden Gate Bridge. Once you arrive in Sausalito, you can explore the town's many art galleries, boutiques, and restaurants. You can also rent a bike and ride along the waterfront.\\n3. Visit the California Academy of Sciences: The California Academy of Sciences is a natural history museum located in Golden Gate Park. It features an aquarium, a planetarium, a rainforest dome, and a natural history museum. You can learn about the diverse ecosystems of California, see penguins and other marine life, and take a journey through the stars in the planetarium. It's a great place to spend a day exploring and learning about the natural world.\\n\\nI hope these suggestions are helpful and that you have a great time in San Francisco!\",\n",
    "                30,\n",
    "                353,\n",
    "            ),\n",
    "            (\n",
    "                \"San Francisco is a vibrant city with endless options for entertainment and exploration. Here are 3 fun things to do in SF:\\n\\n1. **Explore Fisherman's Wharf and Pier 39**: This iconic waterfront district is known for its stunning views of the Golden Gate Bridge, sea lions at Pier 39, and a variety of seafood restaurants. You can also take a stroll along the pier, visit the Aquarium of the Bay, or ride the historic cable cars.\\n\\n2. **Take a stroll across the Golden Gate Bridge**: This iconic suspension bridge is a must-visit attraction in SF. You can walk, bike, or drive across the bridge for breathtaking views of the San Francisco Bay, Alcatraz Island, and the city skyline. If you're feeling adventurous, you can even take a guided tour or bike ride across the bridge.\\n\\n3. **Visit Alcatraz Island**: Once a notorious maximum-security prison, Alcatraz Island is now a popular tourist attraction. Take a ferry to the island and explore the former prison cells, listen to a guided audio tour, or take a self-guided tour of the island. You can also see the famous \\\"Cell Block C\\\" and learn about the island's infamous history.\\n\\nThese are just a few of the many fun things to do in SF. Whether you're interested in history, nature, or entertainment, there's something for everyone in this vibrant city!\",\n",
    "                49,\n",
    "                284,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "791537f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 9;\n            var nbb_formatted_code = \"def backoff_mechanism(\\n    func, modelId, inference_params, maxTokens, messages, prev_response, debug=True\\n):\\n    MAX_RETRIES = 5  # Maximum number of retries\\n    INITIAL_DELAY = 1  # Initial delay in seconds\\n    MAX_DELAY = 60  # Maximum delay in second\\n\\n    delay = INITIAL_DELAY\\n    retries = 0\\n\\n    while retries < MAX_RETRIES:\\n        try:\\n            return func(\\n                modelId=modelId,\\n                inference_params=inference_params,\\n                maxTokens=maxTokens,\\n                messages=messages,\\n                prev_response=prev_response,\\n                debug=debug,\\n            )\\n        except ClientError as exception_obj:\\n            if exception_obj.response[\\\"Error\\\"][\\\"Code\\\"] == \\\"ThrottlingException\\\":\\n                print(f\\\"Retry {retries + 1}/{MAX_RETRIES}\\\")\\n                time.sleep(delay + random.uniform(0, 1))  # Add a random jitter\\n                delay = min(delay * 2, MAX_DELAY)\\n                retries += 1\\n            else:\\n                raise\\n    else:\\n        print(\\\"Max retries reached!\\\")\\n        return \\\"\\\", 0, 0\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def backoff_mechanism(\n",
    "    func, modelId, inference_params, maxTokens, messages, prev_response, debug=True\n",
    "):\n",
    "    MAX_RETRIES = 5  # Maximum number of retries\n",
    "    INITIAL_DELAY = 1  # Initial delay in seconds\n",
    "    MAX_DELAY = 60  # Maximum delay in second\n",
    "\n",
    "    delay = INITIAL_DELAY\n",
    "    retries = 0\n",
    "\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            return func(\n",
    "                modelId=modelId,\n",
    "                inference_params=inference_params,\n",
    "                maxTokens=maxTokens,\n",
    "                messages=messages,\n",
    "                prev_response=prev_response,\n",
    "                debug=debug,\n",
    "            )\n",
    "        except ClientError as exception_obj:\n",
    "            if exception_obj.response[\"Error\"][\"Code\"] == \"ThrottlingException\":\n",
    "                print(f\"Retry {retries + 1}/{MAX_RETRIES}\")\n",
    "                time.sleep(delay + random.uniform(0, 1))  # Add a random jitter\n",
    "                delay = min(delay * 2, MAX_DELAY)\n",
    "                retries += 1\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        print(\"Max retries reached!\")\n",
    "        return \"\", 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "603c3e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 10;\n            var nbb_formatted_code = \"def invoke_model(\\n    modelId, inference_params, maxTokens, messages, prev_response, debug=True\\n):\\n    bedrock = Session().client(\\n        service_name=\\\"bedrock-runtime\\\",\\n    )\\n\\n    if debug:\\n        print(f\\\"Invoking {modelId}\\\")\\n\\n    system_prompt = (\\n        getFinalSystemPrompt(aggreagator_system_prompt, prev_response)\\n        if prev_response\\n        else []\\n    )\\n\\n    if system_prompt is None:\\n        return \\\"\\\", 0, 0\\n\\n    respone = None\\n    if modelId in (\\n        \\\"mistral.mistral-7b-instruct-v0:2\\\",\\n        \\\"mistral.mixtral-8x7b-instruct-v0:1\\\",\\n    ):\\n\\n        if prev_response:\\n            messages[0][\\\"content\\\"][0][\\n                \\\"text\\\"\\n            ] = f\\\"<s>[INST] {system_prompt} \\\\n\\\\n {messages[0]['content'][0]['text']} [/INST]\\\"\\n        else:\\n            messages[0][\\\"content\\\"][0][\\n                \\\"text\\\"\\n            ] = f\\\"<s>[INST] {messages[0]['content'][0]['text']} [/INST]\\\"\\n\\n        response = bedrock.converse(\\n            modelId=modelId,\\n            messages=messages,\\n            inferenceConfig={\\n                \\\"maxTokens\\\": maxTokens,\\n                \\\"temperature\\\": inference_params[\\\"temperature\\\"],\\n                \\\"topP\\\": inference_params[\\\"topP\\\"],\\n            },\\n            additionalModelRequestFields={\\n                f\\\"{key}\\\": inference_params[key]\\n                for key in inference_params.keys()\\n                if key not in (\\\"temperature\\\", \\\"topP\\\")\\n            },\\n        )\\n    else:\\n\\n        response = bedrock.converse(\\n            modelId=modelId,\\n            messages=messages,\\n            system=([{\\\"text\\\": system_prompt}] if prev_response else []),\\n            inferenceConfig={\\n                \\\"maxTokens\\\": maxTokens,\\n                \\\"temperature\\\": inference_params[\\\"temperature\\\"],\\n                \\\"topP\\\": inference_params[\\\"topP\\\"],\\n            },\\n            additionalModelRequestFields={\\n                f\\\"{key}\\\": inference_params[key]\\n                for key in inference_params.keys()\\n                if key not in (\\\"temperature\\\", \\\"topP\\\")\\n            },\\n        )\\n\\n    return (\\n        response[\\\"output\\\"][\\\"message\\\"][\\\"content\\\"][0][\\\"text\\\"],\\n        response[\\\"usage\\\"][\\\"inputTokens\\\"],\\n        response[\\\"usage\\\"][\\\"outputTokens\\\"],\\n    )\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def invoke_model(\n",
    "    modelId, inference_params, maxTokens, messages, prev_response, debug=True\n",
    "):\n",
    "    bedrock = Session().client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Invoking {modelId}\")\n",
    "\n",
    "    system_prompt = (\n",
    "        getFinalSystemPrompt(aggreagator_system_prompt, prev_response)\n",
    "        if prev_response\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if system_prompt is None:\n",
    "        return \"\", 0, 0\n",
    "\n",
    "    respone = None\n",
    "    if modelId in (\n",
    "        \"mistral.mistral-7b-instruct-v0:2\",\n",
    "        \"mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "    ):\n",
    "\n",
    "        if prev_response:\n",
    "            messages[0][\"content\"][0][\n",
    "                \"text\"\n",
    "            ] = f\"<s>[INST] {system_prompt} \\n\\n {messages[0]['content'][0]['text']} [/INST]\"\n",
    "        else:\n",
    "            messages[0][\"content\"][0][\n",
    "                \"text\"\n",
    "            ] = f\"<s>[INST] {messages[0]['content'][0]['text']} [/INST]\"\n",
    "\n",
    "        response = bedrock.converse(\n",
    "            modelId=modelId,\n",
    "            messages=messages,\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": maxTokens,\n",
    "                \"temperature\": inference_params[\"temperature\"],\n",
    "                \"topP\": inference_params[\"topP\"],\n",
    "            },\n",
    "            additionalModelRequestFields={\n",
    "                f\"{key}\": inference_params[key]\n",
    "                for key in inference_params.keys()\n",
    "                if key not in (\"temperature\", \"topP\")\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        response = bedrock.converse(\n",
    "            modelId=modelId,\n",
    "            messages=messages,\n",
    "            system=([{\"text\": system_prompt}] if prev_response else []),\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": maxTokens,\n",
    "                \"temperature\": inference_params[\"temperature\"],\n",
    "                \"topP\": inference_params[\"topP\"],\n",
    "            },\n",
    "            additionalModelRequestFields={\n",
    "                f\"{key}\": inference_params[key]\n",
    "                for key in inference_params.keys()\n",
    "                if key not in (\"temperature\", \"topP\")\n",
    "            },\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        response[\"output\"][\"message\"][\"content\"][0][\"text\"],\n",
    "        response[\"usage\"][\"inputTokens\"],\n",
    "        response[\"usage\"][\"outputTokens\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ae1ead9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Hello! I\\'m Claude, an AI assistant created by Anthropic. I don\\'t actually have a name like \"Sonnet.\" How can I help you today?',\n",
       " 15,\n",
       " 38)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 11;\n            var nbb_formatted_code = \"prompt = \\\"Hi, Sonnet\\\"\\nmessages = [{\\\"role\\\": \\\"user\\\", \\\"content\\\": [{\\\"text\\\": f\\\"User Query: {prompt}\\\"}]}]\\nbackoff_mechanism(\\n    func=invoke_model,\\n    modelId=\\\"anthropic.claude-3-sonnet-20240229-v1:0\\\",\\n    inference_params={\\\"temperature\\\": 0.5, \\\"topP\\\": 1.0, \\\"top_k\\\": 250},\\n    maxTokens=512,\\n    prev_response=\\\"\\\",\\n    messages=messages,\\n)\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Hi, Sonnet\"\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": f\"User Query: {prompt}\"}]}]\n",
    "backoff_mechanism(\n",
    "    func=invoke_model,\n",
    "    modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    inference_params={\"temperature\": 0.5, \"topP\": 1.0, \"top_k\": 250},\n",
    "    maxTokens=512,\n",
    "    prev_response=\"\",\n",
    "    messages=messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1b99b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 12;\n            var nbb_formatted_code = \"async def run_llms(\\n    modelId, inference_params, maxTokens, messages, prev_response=None, debug=True\\n):\\n    \\\"\\\"\\\"Runs a parrallel LLM call to multiple models while accounting for previous responses + rate limits.\\\"\\\"\\\"\\n    return await asyncio.to_thread(\\n        backoff_mechanism,\\n        invoke_model,\\n        modelId,\\n        inference_params,\\n        maxTokens,\\n        messages,\\n        prev_response,\\n        debug,\\n    )\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async def run_llms(\n",
    "    modelId, inference_params, maxTokens, messages, prev_response=None, debug=True\n",
    "):\n",
    "    \"\"\"Runs a parrallel LLM call to multiple models while accounting for previous responses + rate limits.\"\"\"\n",
    "    return await asyncio.to_thread(\n",
    "        backoff_mechanism,\n",
    "        invoke_model,\n",
    "        modelId,\n",
    "        inference_params,\n",
    "        maxTokens,\n",
    "        messages,\n",
    "        prev_response,\n",
    "        debug,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca93896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Hello! I\\'m Claude, an AI assistant created by Anthropic. I don\\'t have a specific name like \"Sonnet\", but I\\'m happy to chat with you. How can I help you today?',\n",
       " 15,\n",
       " 47)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 13;\n            var nbb_formatted_code = \"await run_llms(\\n    modelId=\\\"anthropic.claude-3-sonnet-20240229-v1:0\\\",\\n    inference_params={\\\"temperature\\\": 0.5, \\\"topP\\\": 1.0, \\\"top_k\\\": 250},\\n    maxTokens=512,\\n    prev_response=\\\"\\\",\\n    messages=messages,\\n)\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await run_llms(\n",
    "    modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    inference_params={\"temperature\": 0.5, \"topP\": 1.0, \"top_k\": 250},\n",
    "    maxTokens=512,\n",
    "    prev_response=\"\",\n",
    "    messages=messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff419c",
   "metadata": {},
   "source": [
    "## Mixture of Agents - Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "819e8a24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking anthropic.claude-3-haiku-20240307-v1:0\n",
      "Invoking mistral.mixtral-8x7b-instruct-v0:1\n",
      "Invoking us.meta.llama3-2-3b-instruct-v1:0\n",
      "Invoking us.meta.llama3-2-3b-instruct-v1:0\n",
      "Invoking mistral.mixtral-8x7b-instruct-v0:1\n",
      "Invoking anthropic.claude-3-haiku-20240307-v1:0\n",
      "Invoking anthropic.claude-3-haiku-20240307-v1:0\n",
      "Final Response:\n",
      "\n",
      "After carefully reviewing the responses provided by the various open-source models, I have synthesized the information into a comprehensive and accurate response to your query about fun things to do in San Francisco.\n",
      "\n",
      "1. Visit the iconic Golden Gate Bridge: This renowned landmark is a must-see attraction in San Francisco. You can walk, bike, or drive across the bridge to take in the stunning views of the bay and the city skyline. The bridge offers a classic San Francisco experience and provides excellent photo opportunities.\n",
      "\n",
      "2. Explore Fisherman's Wharf and Pier 39: This lively waterfront district is filled with seafood restaurants, street performers, and popular attractions like the Musée Mécanique arcade and the Aquarium of the Bay. You can also see the famous sea lions that reside at Pier 39, making it a fun and engaging destination for visitors.\n",
      "\n",
      "3. Ride the historic cable cars: San Francisco's cable car system is a unique and iconic mode of transportation that offers a delightful way to experience the city. You can ride the Powell-Mason or Powell-Hyde lines, which take you through some of the city's most iconic neighborhoods, including Nob Hill and Chinatown.\n",
      "\n",
      "In addition to these top three activities, San Francisco offers a wealth of other exciting experiences, such as visiting the vibrant Mission District, touring the former prison on Alcatraz Island, and exploring the diverse neighborhoods like Chinatown and North Beach. The city is renowned for its rich history, stunning architecture, and diverse cultural offerings, making it a truly captivating destination for visitors.\n",
      "\n",
      "I hope this synthesized response provides you with a comprehensive and accurate overview of some of the most fun and engaging activities to enjoy in San Francisco. Please let me know if you have any other questions!\n",
      "Input Token Usage: 6217, Output Token Usage: 2231\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 14;\n            var nbb_formatted_code = \"async def main():\\n    \\\"\\\"\\\"Run the main loop of the MOA process.\\\"\\\"\\\"\\n    user_prompt = \\\"What are 3 fun things to do in SF?\\\"\\n\\n    messages = [{\\\"role\\\": \\\"user\\\", \\\"content\\\": [{\\\"text\\\": f\\\"User Query: {user_prompt}\\\"}]}]\\n\\n    input_token_usage = 0\\n    output_token_usage = 0\\n\\n    # Invoke Layer-1 of MoA\\n    results = await asyncio.gather(\\n        *[\\n            run_llms(\\n                modelId=reference_model[\\\"modelId\\\"],\\n                inference_params=reference_model[\\\"inference_params\\\"],\\n                maxTokens=reference_model[\\\"maxTokens\\\"],\\n                messages=messages,\\n                prev_response=None,\\n            )\\n            for reference_model in reference_models\\n        ]\\n    )\\n\\n    input_token_usage += sum([element[1] for element in results])\\n    output_token_usage += sum([element[2] for element in results])\\n    # Invoke Layer-2, Layer-3, .... Layer-(N-1) of MoA\\n    for _ in range(1, layers - 1):\\n        results = await asyncio.gather(\\n            *[\\n                run_llms(\\n                    modelId=reference_model[\\\"modelId\\\"],\\n                    inference_params=reference_model[\\\"inference_params\\\"],\\n                    maxTokens=reference_model[\\\"maxTokens\\\"],\\n                    messages=messages,\\n                    prev_response=results,\\n                )\\n                for reference_model in reference_models\\n            ]\\n        )\\n        input_token_usage += sum([element[1] for element in results])\\n        output_token_usage += sum([element[2] for element in results])\\n\\n    bedrock = Session().client(\\n        service_name=\\\"bedrock-runtime\\\",\\n    )\\n\\n    # Invoke the Aggregator model\\n    response = backoff_mechanism(\\n        func=invoke_model,\\n        modelId=aggregator_model[\\\"modelId\\\"],\\n        inference_params=aggregator_model[\\\"inference_params\\\"],\\n        maxTokens=aggregator_model[\\\"maxTokens\\\"],\\n        messages=messages,\\n        prev_response=results,\\n    )\\n\\n    input_token_usage += response[1]\\n    output_token_usage += response[2]\\n\\n    print(f\\\"Final Response:\\\\n\\\\n{response[0]}\\\")\\n    print(\\n        f\\\"Input Token Usage: {input_token_usage}, Output Token Usage: {output_token_usage}\\\"\\n    )\\n\\n\\nawait main()\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async def main():\n",
    "    \"\"\"Run the main loop of the MOA process.\"\"\"\n",
    "    user_prompt = \"What are 3 fun things to do in SF?\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": f\"User Query: {user_prompt}\"}]}]\n",
    "\n",
    "    input_token_usage = 0\n",
    "    output_token_usage = 0\n",
    "\n",
    "    # Invoke Layer-1 of MoA\n",
    "    results = await asyncio.gather(\n",
    "        *[\n",
    "            run_llms(\n",
    "                modelId=reference_model[\"modelId\"],\n",
    "                inference_params=reference_model[\"inference_params\"],\n",
    "                maxTokens=reference_model[\"maxTokens\"],\n",
    "                messages=messages,\n",
    "                prev_response=None,\n",
    "            )\n",
    "            for reference_model in reference_models\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    input_token_usage += sum([element[1] for element in results])\n",
    "    output_token_usage += sum([element[2] for element in results])\n",
    "    # Invoke Layer-2, Layer-3, .... Layer-(N-1) of MoA\n",
    "    for _ in range(1, layers - 1):\n",
    "        results = await asyncio.gather(\n",
    "            *[\n",
    "                run_llms(\n",
    "                    modelId=reference_model[\"modelId\"],\n",
    "                    inference_params=reference_model[\"inference_params\"],\n",
    "                    maxTokens=reference_model[\"maxTokens\"],\n",
    "                    messages=messages,\n",
    "                    prev_response=results,\n",
    "                )\n",
    "                for reference_model in reference_models\n",
    "            ]\n",
    "        )\n",
    "        input_token_usage += sum([element[1] for element in results])\n",
    "        output_token_usage += sum([element[2] for element in results])\n",
    "\n",
    "    bedrock = Session().client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "    )\n",
    "\n",
    "    # Invoke the Aggregator model\n",
    "    response = backoff_mechanism(\n",
    "        func=invoke_model,\n",
    "        modelId=aggregator_model[\"modelId\"],\n",
    "        inference_params=aggregator_model[\"inference_params\"],\n",
    "        maxTokens=aggregator_model[\"maxTokens\"],\n",
    "        messages=messages,\n",
    "        prev_response=results,\n",
    "    )\n",
    "\n",
    "    input_token_usage += response[1]\n",
    "    output_token_usage += response[2]\n",
    "\n",
    "    print(f\"Final Response:\\n\\n{response[0]}\")\n",
    "    print(\n",
    "        f\"Input Token Usage: {input_token_usage}, Output Token Usage: {output_token_usage}\"\n",
    "    )\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be6e253",
   "metadata": {},
   "source": [
    "## Evaluation - AlpacaEval 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3d29a",
   "metadata": {},
   "source": [
    "We mainly evaluate models on AlpacaEval 2.0, a leading benchmark for assessing the alignment of LLMs with human preferences. It contains 805 instructions representative of real use cases. Each model’s response is directly compared against that of the Sonnet 3.5, with a GPT-4-based evaluator determining the likelihood of preferring the\n",
    "evaluated model’s response. To ensure fairness, the evaluation employs length-controlled (LC) win rates, effectively neutralizing length bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f5710",
   "metadata": {},
   "source": [
    "AlpacaEval 2.0 with length-controlled win-rates (paper) has a spearman correlation of 0.98 with ChatBot Arena while costing less than $10 of OpenAI credits run and running in less than 3 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94935994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 15;\n            var nbb_formatted_code = \"eval_set = datasets.load_dataset(\\n    \\\"tatsu-lab/alpaca_eval\\\", \\\"alpaca_eval_gpt4_baseline\\\", trust_remote_code=True\\n)[\\\"eval\\\"]\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_set = datasets.load_dataset(\n",
    "    \"tatsu-lab/alpaca_eval\", \"alpaca_eval_gpt4_baseline\", trust_remote_code=True\n",
    ")[\"eval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baf715c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 16;\n            var nbb_formatted_code = \"eval_set = eval_set.remove_columns([\\\"output\\\", \\\"generator\\\"])\\neval_set = eval_set.to_list()\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_set = eval_set.remove_columns([\"output\", \"generator\"])\n",
    "eval_set = eval_set.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1657677c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 17;\n            var nbb_formatted_code = \"async def invoke_moa(\\n    item, reference_models, aggregator_model, layers, experimentation_round, debug=True\\n):\\n    messages = [\\n        {\\\"role\\\": \\\"user\\\", \\\"content\\\": [{\\\"text\\\": f\\\"User Query: {item['instruction']}\\\"}]}\\n    ]\\n\\n    input_token_usage = 0\\n    output_token_usage = 0\\n    start_time = time.perf_counter()\\n\\n    results = await asyncio.gather(\\n        *[\\n            run_llms(\\n                modelId=reference_model[\\\"modelId\\\"],\\n                inference_params=reference_model[\\\"inference_params\\\"],\\n                maxTokens=reference_model[\\\"maxTokens\\\"],\\n                messages=messages,\\n                prev_response=None,\\n                debug=debug,\\n            )\\n            for reference_model in reference_models\\n        ]\\n    )\\n\\n    input_token_usage += sum([element[1] for element in results])\\n    output_token_usage += sum([element[2] for element in results])\\n\\n    for _ in range(1, layers - 1):\\n        results = await asyncio.gather(\\n            *[\\n                run_llms(\\n                    modelId=reference_model[\\\"modelId\\\"],\\n                    inference_params=reference_model[\\\"inference_params\\\"],\\n                    maxTokens=reference_model[\\\"maxTokens\\\"],\\n                    messages=messages,\\n                    prev_response=results,\\n                    debug=debug,\\n                )\\n                for reference_model in reference_models\\n            ]\\n        )\\n        input_token_usage += sum([element[1] for element in results])\\n        output_token_usage += sum([element[2] for element in results])\\n\\n    bedrock = Session().client(\\n        service_name=\\\"bedrock-runtime\\\",\\n    )\\n    response = backoff_mechanism(\\n        func=invoke_model,\\n        modelId=aggregator_model[\\\"modelId\\\"],\\n        inference_params=aggregator_model[\\\"inference_params\\\"],\\n        maxTokens=aggregator_model[\\\"maxTokens\\\"],\\n        messages=messages,\\n        prev_response=results,\\n        debug=debug,\\n    )\\n\\n    total_time = time.perf_counter() - start_time\\n    input_token_usage += response[1]\\n    output_token_usage += response[2]\\n\\n    return {\\n        **item,\\n        **{\\n            \\\"output\\\": response[0],\\n            \\\"generator\\\": aggregator_model[\\\"modelId\\\"]\\n            + str(layers)\\n            + str(experimentation_round)\\n            + \\\"-moa\\\",\\n        },\\n    }, {\\n        **item,\\n        **{\\n            \\\"output\\\": response[0],\\n            \\\"generator\\\": aggregator_model[\\\"modelId\\\"]\\n            + str(layers)\\n            + str(experimentation_round)\\n            + \\\"-moa\\\",\\n            \\\"input_token_usage\\\": input_token_usage,\\n            \\\"output_token_usage\\\": output_token_usage,\\n            \\\"total_time\\\": total_time,\\n        },\\n    }\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async def invoke_moa(\n",
    "    item, reference_models, aggregator_model, layers, experimentation_round, debug=True\n",
    "):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [{\"text\": f\"User Query: {item['instruction']}\"}]}\n",
    "    ]\n",
    "\n",
    "    input_token_usage = 0\n",
    "    output_token_usage = 0\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    results = await asyncio.gather(\n",
    "        *[\n",
    "            run_llms(\n",
    "                modelId=reference_model[\"modelId\"],\n",
    "                inference_params=reference_model[\"inference_params\"],\n",
    "                maxTokens=reference_model[\"maxTokens\"],\n",
    "                messages=messages,\n",
    "                prev_response=None,\n",
    "                debug=debug,\n",
    "            )\n",
    "            for reference_model in reference_models\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    input_token_usage += sum([element[1] for element in results])\n",
    "    output_token_usage += sum([element[2] for element in results])\n",
    "\n",
    "    for _ in range(1, layers - 1):\n",
    "        results = await asyncio.gather(\n",
    "            *[\n",
    "                run_llms(\n",
    "                    modelId=reference_model[\"modelId\"],\n",
    "                    inference_params=reference_model[\"inference_params\"],\n",
    "                    maxTokens=reference_model[\"maxTokens\"],\n",
    "                    messages=messages,\n",
    "                    prev_response=results,\n",
    "                    debug=debug,\n",
    "                )\n",
    "                for reference_model in reference_models\n",
    "            ]\n",
    "        )\n",
    "        input_token_usage += sum([element[1] for element in results])\n",
    "        output_token_usage += sum([element[2] for element in results])\n",
    "\n",
    "    bedrock = Session().client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "    )\n",
    "    response = backoff_mechanism(\n",
    "        func=invoke_model,\n",
    "        modelId=aggregator_model[\"modelId\"],\n",
    "        inference_params=aggregator_model[\"inference_params\"],\n",
    "        maxTokens=aggregator_model[\"maxTokens\"],\n",
    "        messages=messages,\n",
    "        prev_response=results,\n",
    "        debug=debug,\n",
    "    )\n",
    "\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    input_token_usage += response[1]\n",
    "    output_token_usage += response[2]\n",
    "\n",
    "    return {\n",
    "        **item,\n",
    "        **{\n",
    "            \"output\": response[0],\n",
    "            \"generator\": aggregator_model[\"modelId\"]\n",
    "            + str(layers)\n",
    "            + str(experimentation_round)\n",
    "            + \"-moa\",\n",
    "        },\n",
    "    }, {\n",
    "        **item,\n",
    "        **{\n",
    "            \"output\": response[0],\n",
    "            \"generator\": aggregator_model[\"modelId\"]\n",
    "            + str(layers)\n",
    "            + str(experimentation_round)\n",
    "            + \"-moa\",\n",
    "            \"input_token_usage\": input_token_usage,\n",
    "            \"output_token_usage\": output_token_usage,\n",
    "            \"total_time\": total_time,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "901972e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 18;\n            var nbb_formatted_code = \"response = await invoke_moa(\\n    eval_set[0],\\n    reference_models=reference_models,\\n    aggregator_model=aggregator_model,\\n    layers=2,\\n    experimentation_round=2,\\n    debug=False,\\n)\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await invoke_moa(\n",
    "    eval_set[0],\n",
    "    reference_models=reference_models,\n",
    "    aggregator_model=aggregator_model,\n",
    "    layers=2,\n",
    "    experimentation_round=2,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eb74e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What are the names of some famous actors that started their careers on Broadway?',\n",
       " 'dataset': 'helpful_base',\n",
       " 'output': 'Here is a synthesis of the key information provided in the previous responses about famous actors who started their careers on Broadway:\\n\\nMany acclaimed actors and actresses began their professional acting careers on the Broadway stage before finding success in film and television. Some notable examples include:\\n\\n- Al Pacino - Got his start in Broadway productions like \"The Indian Wants the Bronx\" and \"Does a Tiger Wear a Necktie?\" in the 1960s.\\n\\n- Meryl Streep - Appeared in Broadway shows such as \"Trelawny of the \\'Wells\\'\" and \"Happy End\" in the 1970s.\\n\\n- Robert De Niro - Made his Broadway debut in the 1968 play \"The Wedding Party\" before transitioning to film.\\n\\n- Denzel Washington - Starred in Broadway productions like \"Checkmates\" and \"Julius Caesar\" early in his career.\\n\\n- Viola Davis - Had an extensive theater background, including roles in Broadway plays like \"King Hedley II\" and \"Fences.\"\\n\\n- Kevin Kline, Glenn Close, and Christopher Walken also launched their acting careers on the New York stage before becoming major film and TV stars.\\n\\nOther examples include James Dean, Liza Minnelli, Anne Hathaway, Hugh Jackman, and Whoopi Goldberg, among others. Many of these performers honed their craft on Broadway before finding widespread fame and acclaim in Hollywood.',\n",
       " 'generator': 'anthropic.claude-3-haiku-20240307-v1:022-moa'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 19;\n            var nbb_formatted_code = \"response[0]\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c309db55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What are the names of some famous actors that started their careers on Broadway?',\n",
       " 'dataset': 'helpful_base',\n",
       " 'output': 'Here is a synthesis of the key information provided in the previous responses about famous actors who started their careers on Broadway:\\n\\nMany acclaimed actors and actresses began their professional acting careers on the Broadway stage before finding success in film and television. Some notable examples include:\\n\\n- Al Pacino - Got his start in Broadway productions like \"The Indian Wants the Bronx\" and \"Does a Tiger Wear a Necktie?\" in the 1960s.\\n\\n- Meryl Streep - Appeared in Broadway shows such as \"Trelawny of the \\'Wells\\'\" and \"Happy End\" in the 1970s.\\n\\n- Robert De Niro - Made his Broadway debut in the 1968 play \"The Wedding Party\" before transitioning to film.\\n\\n- Denzel Washington - Starred in Broadway productions like \"Checkmates\" and \"Julius Caesar\" early in his career.\\n\\n- Viola Davis - Had an extensive theater background, including roles in Broadway plays like \"King Hedley II\" and \"Fences.\"\\n\\n- Kevin Kline, Glenn Close, and Christopher Walken also launched their acting careers on the New York stage before becoming major film and TV stars.\\n\\nOther examples include James Dean, Liza Minnelli, Anne Hathaway, Hugh Jackman, and Whoopi Goldberg, among others. Many of these performers honed their craft on Broadway before finding widespread fame and acclaim in Hollywood.',\n",
       " 'generator': 'anthropic.claude-3-haiku-20240307-v1:022-moa',\n",
       " 'input_token_usage': 1512,\n",
       " 'output_token_usage': 1484,\n",
       " 'total_time': 11.312489481992088}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 20;\n            var nbb_formatted_code = \"response[1]\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1ab1481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0!\n",
      "Completed 5!\n",
      "Completed 10!\n",
      "Completed 15!\n",
      "Completed 20!\n",
      "Completed 25!\n",
      "Completed 30!\n",
      "Completed 35!\n",
      "Completed 40!\n",
      "Completed 45!\n",
      "Completed 50!\n",
      "Completed 55!\n",
      "Completed 60!\n",
      "Completed 65!\n",
      "Completed 70!\n",
      "Completed 75!\n",
      "Completed 80!\n",
      "Completed 85!\n",
      "Completed 90!\n",
      "Completed 95!\n",
      "Completed 100!\n",
      "Completed 105!\n",
      "Completed 110!\n",
      "Completed 115!\n",
      "Completed 120!\n",
      "Completed 125!\n",
      "Completed 130!\n",
      "Completed 135!\n",
      "Completed 140!\n",
      "Completed 145!\n",
      "Completed 150!\n",
      "Completed 155!\n",
      "Completed 160!\n",
      "Completed 165!\n",
      "Completed 170!\n",
      "Completed 175!\n",
      "Completed 180!\n",
      "Completed 185!\n",
      "Completed 190!\n",
      "Completed 195!\n",
      "Completed 200!\n",
      "Completed 205!\n",
      "Completed 210!\n",
      "Completed 215!\n",
      "Completed 220!\n",
      "Completed 225!\n",
      "Completed 230!\n",
      "Completed 235!\n",
      "Completed 240!\n",
      "Completed 245!\n",
      "Completed 250!\n",
      "Completed 255!\n",
      "Completed 260!\n",
      "Completed 265!\n",
      "Completed 270!\n",
      "Completed 275!\n",
      "Completed 280!\n",
      "Completed 285!\n",
      "Completed 290!\n",
      "Completed 295!\n",
      "Completed 300!\n",
      "Completed 305!\n",
      "Completed 310!\n",
      "Completed 315!\n",
      "Completed 320!\n",
      "Completed 325!\n",
      "Completed 330!\n",
      "Completed 335!\n",
      "Completed 340!\n",
      "Completed 345!\n",
      "Completed 350!\n",
      "Completed 355!\n",
      "Completed 360!\n",
      "Completed 365!\n",
      "Completed 370!\n",
      "Completed 375!\n",
      "Completed 380!\n",
      "Completed 385!\n",
      "Completed 390!\n",
      "Completed 395!\n",
      "Completed 400!\n",
      "Completed 405!\n",
      "Completed 410!\n",
      "Completed 415!\n",
      "Completed 420!\n",
      "Completed 425!\n",
      "Completed 430!\n",
      "Completed 435!\n",
      "Completed 440!\n",
      "Completed 445!\n",
      "Completed 450!\n",
      "Completed 455!\n",
      "Completed 460!\n",
      "Completed 465!\n",
      "Completed 470!\n",
      "Completed 475!\n",
      "Completed 480!\n",
      "Completed 485!\n",
      "Completed 490!\n",
      "Completed 495!\n",
      "Completed 500!\n",
      "Completed 505!\n",
      "Completed 510!\n",
      "Completed 515!\n",
      "Completed 520!\n",
      "Completed 525!\n",
      "Completed 530!\n",
      "Completed 535!\n",
      "Completed 540!\n",
      "Completed 545!\n",
      "Completed 550!\n",
      "Completed 555!\n",
      "Completed 560!\n",
      "Completed 565!\n",
      "Completed 570!\n",
      "Completed 575!\n",
      "Completed 580!\n",
      "Completed 585!\n",
      "Completed 590!\n",
      "Completed 595!\n",
      "Completed 600!\n",
      "Completed 605!\n",
      "Completed 610!\n",
      "Completed 615!\n",
      "Completed 620!\n",
      "Completed 625!\n",
      "Completed 630!\n",
      "Completed 635!\n",
      "Completed 640!\n",
      "Completed 645!\n",
      "Completed 650!\n",
      "Completed 655!\n",
      "Completed 660!\n",
      "Completed 665!\n",
      "Completed 670!\n",
      "Completed 675!\n",
      "Completed 680!\n",
      "Completed 685!\n",
      "Completed 690!\n",
      "Completed 695!\n",
      "Completed 700!\n",
      "Completed 705!\n",
      "Completed 710!\n",
      "Completed 715!\n",
      "Completed 720!\n",
      "Completed 725!\n",
      "Completed 730!\n",
      "Completed 735!\n",
      "Completed 740!\n",
      "Completed 745!\n",
      "Completed 750!\n",
      "Completed 755!\n",
      "Completed 760!\n",
      "Completed 765!\n",
      "Completed 770!\n",
      "Completed 775!\n",
      "Completed 780!\n",
      "Completed 785!\n",
      "Completed 790!\n",
      "Completed 795!\n",
      "Completed 800!\n",
      "Total time: 7040.109539813013 seconds\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 21;\n            var nbb_formatted_code = \"experimentation_round = 12\\n\\n\\nasync def eval_moa(experimentation_round=experimentation_round):\\n\\n    new_eval_set = list()\\n    extended_eval_set = list()\\n    start_time = time.perf_counter()\\n\\n    input_token_usage = 0\\n    output_token_usage = 0\\n\\n    for idx in range(0, len(eval_set), 5):\\n        results = await asyncio.gather(\\n            *[\\n                invoke_moa(\\n                    item,\\n                    reference_models=reference_models,\\n                    aggregator_model=aggregator_model,\\n                    layers=layers,\\n                    experimentation_round=experimentation_round,\\n                    debug=False,\\n                )\\n                for item in eval_set[idx : idx + 5]\\n            ]\\n        )\\n\\n        new_eval_set_items = [result[0] for result in results]\\n        extended_eval_set_items = [result[1] for result in results]\\n\\n        # Wait for the futures to complete and collect the results\\n        new_eval_set.extend(new_eval_set_items)\\n        extended_eval_set.extend(extended_eval_set_items)\\n        print(f\\\"Completed {idx}!\\\")\\n\\n    total_time = time.perf_counter() - start_time\\n    print(f\\\"Total time: {total_time} seconds\\\")\\n\\n    with open(\\n        f\\\"outputs/{aggregator_model['modelId']}-moa-extended-eval-set-round-{experimentation_round}.json\\\",\\n        \\\"w\\\",\\n    ) as f:\\n        json.dump(list(extended_eval_set), f, indent=2)\\n\\n    with open(\\n        f\\\"outputs/{aggregator_model['modelId']}-moa-round-{experimentation_round}.json\\\",\\n        \\\"w\\\",\\n    ) as f:\\n        json.dump(list(new_eval_set), f, indent=2)\\n\\n\\nawait eval_moa()\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experimentation_round = 11\n",
    "\n",
    "\n",
    "async def eval_moa(experimentation_round=experimentation_round):\n",
    "\n",
    "    new_eval_set = list()\n",
    "    extended_eval_set = list()\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    input_token_usage = 0\n",
    "    output_token_usage = 0\n",
    "\n",
    "    for idx in range(0, len(eval_set), 5):\n",
    "        results = await asyncio.gather(\n",
    "            *[\n",
    "                invoke_moa(\n",
    "                    item,\n",
    "                    reference_models=reference_models,\n",
    "                    aggregator_model=aggregator_model,\n",
    "                    layers=layers,\n",
    "                    experimentation_round=experimentation_round,\n",
    "                    debug=False,\n",
    "                )\n",
    "                for item in eval_set[idx : idx + 5]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        new_eval_set_items = [result[0] for result in results]\n",
    "        extended_eval_set_items = [result[1] for result in results]\n",
    "\n",
    "        # Wait for the futures to complete and collect the results\n",
    "        new_eval_set.extend(new_eval_set_items)\n",
    "        extended_eval_set.extend(extended_eval_set_items)\n",
    "        print(f\"Completed {idx}!\")\n",
    "\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    print(f\"Total time: {total_time} seconds\")\n",
    "\n",
    "    with open(\n",
    "        f\"outputs/{aggregator_model['modelId']}-moa-extended-eval-set-round-{experimentation_round}.json\",\n",
    "        \"w\",\n",
    "    ) as f:\n",
    "        json.dump(list(extended_eval_set), f, indent=2)\n",
    "\n",
    "    with open(\n",
    "        f\"outputs/{aggregator_model['modelId']}-moa-round-{experimentation_round}.json\",\n",
    "        \"w\",\n",
    "    ) as f:\n",
    "        json.dump(list(new_eval_set), f, indent=2)\n",
    "\n",
    "\n",
    "await eval_moa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5bf69d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 805 outputs.\n",
      "All outputs are of length greater than 0.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 22;\n            var nbb_formatted_code = \"with open(\\n    f\\\"outputs/{aggregator_model['modelId']}-moa-round-{experimentation_round}.json\\\"\\n) as f:\\n    outputs = json.loads(f.read())\\n\\ntry:\\n    assert len(outputs) == len(eval_set)\\n    print(f\\\"We have a total of {len(outputs)} outputs.\\\")\\n\\n    for output in outputs:\\n        assert len(output[\\\"output\\\"]) != 0\\n\\n    print(f\\\"All outputs are of length greater than 0.\\\")\\n\\nexcept:\\n    assert len(outputs) == len(eval_set)\\n    print(\\\"Not all outputs are of length greater than 0.\\\")\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\n",
    "    f\"outputs/{aggregator_model['modelId']}-moa-round-{experimentation_round}.json\"\n",
    ") as f:\n",
    "    outputs = json.loads(f.read())\n",
    "\n",
    "try:\n",
    "    assert len(outputs) == len(eval_set)\n",
    "    print(f\"We have a total of {len(outputs)} outputs.\")\n",
    "\n",
    "    for output in outputs:\n",
    "        assert len(output[\"output\"]) != 0\n",
    "\n",
    "    print(f\"All outputs are of length greater than 0.\")\n",
    "\n",
    "except:\n",
    "    assert len(outputs) == len(eval_set)\n",
    "    print(\"Not all outputs are of length greater than 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a90b026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating the anthropic.claude-3-haiku-20240307-v1:0211-moa outputs.\n",
      "INFO:root:Creating the annotator from `weighted_alpaca_eval_gpt4_turbo`.\n",
      "INFO:root:Saving annotations to `/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json`.\n",
      "INFO:root:Loading all annotations from /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk:   0%|                                   | 0/7 [00:00<?, ?it/s]INFO:root:Annotating 0 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Saving all annotations to /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk:  14%|███▊                       | 1/7 [00:01<00:07,  1.21s/it]INFO:root:Annotating 0 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Saving all annotations to /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk:  29%|███████▋                   | 2/7 [00:02<00:05,  1.15s/it]INFO:root:Annotating 0 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Saving all annotations to /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk:  43%|███████████▌               | 3/7 [00:03<00:04,  1.13s/it]INFO:root:Annotating 0 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Saving all annotations to /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk:  57%|███████████████▍           | 4/7 [00:04<00:03,  1.08s/it]INFO:root:Annotating 0 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Saving all annotations to /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk:  71%|███████████████████▎       | 5/7 [00:05<00:02,  1.15s/it]INFO:root:Annotating 0 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Saving all annotations to /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk:  86%|███████████████████████▏   | 6/7 [00:06<00:01,  1.12s/it]INFO:root:Annotating 0 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Saving all annotations to /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json.\n",
      "Annotation chunk: 100%|███████████████████████████| 7/7 [00:07<00:00,  1.13s/it]\n",
      "INFO:root:drop 34 outputs that are not preferences\n",
      "INFO:root:drop 34 outputs that are not preferences\n",
      "INFO:root:Saving all results to leaderboard/weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Not saving the result to the cached leaderboard because precomputed_leaderboard is not a path but <class 'NoneType'>.\n",
      "                                               length_controlled_winrate  win_rate  standard_error  n_total  avg_length\n",
      "anthropic.claude-3-haiku-20240307-v1:0211-moa                      26.34     34.05            1.47      771        1578\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 37;\n            var nbb_formatted_code = \"!alpaca_eval --model_outputs outputs/anthropic.claude-3-haiku-20240307-v1:0-moa-round-11.json --reference_outputs alpaca_eval/results/claude-3-5-sonnet-20240620/model_outputs.json --output_path leaderboard\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!alpaca_eval --model_outputs outputs/anthropic.claude-3-haiku-20240307-v1:0-moa-round-11.json --reference_outputs alpaca_eval/results/claude-3-5-sonnet-20240620/model_outputs.json --output_path leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ec673",
   "metadata": {},
   "source": [
    "## Evaluation - Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f8718",
   "metadata": {},
   "source": [
    "| Models            | Price per 1,000 input tokens | Price per 1,000 output tokens |\n",
    "| :---------------- | :------: | ----: |\n",
    "| Claude 3 Haiku    |   0.00025   | 0.00125 |\n",
    "| Mixtral 8*7B      |   0.00045   | 0.0007 |\n",
    "| Llama 3.2 Instruct (3B) |  0.00015   | 0.00015 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb6b7650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 45;\n            var nbb_formatted_code = \"with open(\\n    f\\\"outputs/{aggregator_model['modelId']}-moa-extended-eval-set-round-{experimentation_round}.json\\\"\\n) as f:\\n    evaluation_output = pd.DataFrame(json.loads(f.read()))\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\n",
    "    f\"outputs/{aggregator_model['modelId']}-moa-extended-eval-set-round-{experimentation_round}.json\"\n",
    ") as f:\n",
    "    evaluation_output = pd.DataFrame(json.loads(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e112323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 46;\n            var nbb_formatted_code = \"moa_input_token_usage = int(evaluation_output.input_token_usage.sum())\\nmoa_output_token_usage = int(evaluation_output.output_token_usage.sum())\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "moa_input_token_usage = int(evaluation_output.input_token_usage.sum())\n",
    "moa_output_token_usage = int(evaluation_output.output_token_usage.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0daf3423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 47;\n            var nbb_formatted_code = \"total_price_moa = (moa_input_token_usage / 1000) * 0.00045 + (\\n    moa_output_token_usage / 1000\\n) * 0.00125\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_price_moa = (moa_input_token_usage / 1000) * 0.00045 + (\n",
    "    moa_output_token_usage / 1000\n",
    ") * 0.00125"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edaeba6",
   "metadata": {},
   "source": [
    "| Models            | Price per 1,000 input tokens | Price per 1,000 output tokens |\n",
    "| :---------------- | :------: | ----: |\n",
    "| Claude 3.5 Sonnet |   0.003   | 0.015 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9639074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 48;\n            var nbb_formatted_code = \"model_input_token_usage = sum(\\n    evaluation_output.instruction.apply(lambda prompt: client.count_tokens(prompt))\\n)\\nmodel_output_token_usage = sum(\\n    evaluation_output.output.apply(lambda prompt: client.count_tokens(prompt))\\n)\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_input_token_usage = sum(\n",
    "    evaluation_output.instruction.apply(lambda prompt: client.count_tokens(prompt))\n",
    ")\n",
    "model_output_token_usage = sum(\n",
    "    evaluation_output.output.apply(lambda prompt: client.count_tokens(prompt))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d2edccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 49;\n            var nbb_formatted_code = \"total_price_sonnet = (model_input_token_usage / 1000) * 0.003 + (\\n    model_output_token_usage / 1000\\n) * 0.015\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_price_sonnet = (model_input_token_usage / 1000) * 0.003 + (\n",
    "    model_output_token_usage / 1000\n",
    ") * 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f86c42b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total price of running MOA inference on AlpacaEval 2.0 is $1.8397684500000002 and total price of running Anthropic Sonnet 3.5 inference is $4.310700000000001\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 50;\n            var nbb_formatted_code = \"print(\\n    f\\\"Total price of running MOA inference on AlpacaEval 2.0 is ${total_price_moa} and total price of running Anthropic Sonnet 3.5 inference is ${total_price_sonnet}\\\"\\n)\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\n",
    "    f\"Total price of running MOA inference on AlpacaEval 2.0 is ${total_price_moa} and total price of running Anthropic Sonnet 3.5 inference is ${total_price_sonnet}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9766ccf",
   "metadata": {},
   "source": [
    "## Evaluation - Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f047a3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Invoking anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 62;\n            var nbb_formatted_code = \"sonnet_latency = list()\\nfor prompt in evaluation_output[:200].instruction:\\n    messages = [{\\\"role\\\": \\\"user\\\", \\\"content\\\": [{\\\"text\\\": f\\\"User Query: {prompt}\\\"}]}]\\n    start_time = time.perf_counter()\\n    backoff_mechanism(\\n        func=invoke_model,\\n        modelId=\\\"anthropic.claude-3-sonnet-20240229-v1:0\\\",\\n        inference_params={\\\"temperature\\\": 0.5, \\\"topP\\\": 1.0, \\\"top_k\\\": 250},\\n        maxTokens=512,\\n        prev_response=\\\"\\\",\\n        messages=messages,\\n    )\\n    total_time = time.perf_counter() - start_time\\n    sonnet_latency.append(total_time)\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sonnet_latency = list()\n",
    "for prompt in evaluation_output[:200].instruction:\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": f\"User Query: {prompt}\"}]}]\n",
    "    start_time = time.perf_counter()\n",
    "    backoff_mechanism(\n",
    "        func=invoke_model,\n",
    "        modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        inference_params={\"temperature\": 0.5, \"topP\": 1.0, \"top_k\": 250},\n",
    "        maxTokens=512,\n",
    "        prev_response=\"\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    sonnet_latency.append(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3e01084c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        setTimeout(function() {\n            var nbb_cell_id = 79;\n            var nbb_formatted_code = \"fig, ax = plt.subplots()\\nax.bar(\\n    [1, 2],\\n    [\\n        sum(sonnet_latency) / len(sonnet_latency),\\n        sum(evaluation_output[:200].total_time) / len(evaluation_output[:200]),\\n    ],\\n    tick_label=[\\\"Single Model latency\\\", \\\"MoA latency\\\"],\\n    align=\\\"center\\\",\\n)\\nax.set_xlabel(\\\"Latency in seconds\\\")\\nplt.show()\";\n            var nbb_cells = Jupyter.notebook.get_cells();\n            for (var i = 0; i < nbb_cells.length; ++i) {\n                if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                    nbb_cells[i].set_text(nbb_formatted_code);\n                    break;\n                }\n            }\n        }, 500);\n        ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(\n",
    "    [1, 2],\n",
    "    [\n",
    "        sum(sonnet_latency) / len(sonnet_latency),\n",
    "        sum(evaluation_output[:200].total_time) / len(evaluation_output[:200]),\n",
    "    ],\n",
    "    tick_label=[\"Single Model latency\", \"MoA latency\"],\n",
    "    align=\"center\",\n",
    ")\n",
    "ax.set_xlabel(\"Latency in seconds\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
